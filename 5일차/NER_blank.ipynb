{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NER_blank.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iE84wKIhPFRh","executionInfo":{"status":"ok","timestamp":1657694518750,"user_tz":-540,"elapsed":19373,"user":{"displayName":"류상연/AI·소프트웨어학부(소프트웨어전공)","userId":"00705256891800395029"}},"outputId":"74db9a9a-0c3c-4160-ca36-5011883d20f3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["#NER\n","각 단어가 어떠한 개체명(장소, 시간, 등)을 가르키는지를 확인하는 task  \n","dataset:https://www.kaggle.com/datasets/debasisdotcom/name-entity-recognition-ner-dataset\n","\n","##Data PreProcessing  \n","제공된 data는 이미 tokenization이 되어있는 형태로 제공된다.  \n","그렇기에 vocab dictionary를 직접 구성하고 각 tokenization된 token들을 indexing하여 숫자형태의 데이터로 변환환다.  \n","이때 class에 index를 부여해주는 scikit learn의 LabelEncoder를 사용해서 변환환다.  \n","또한 모든 token의 수를일정하게 맞춰주어야만 tensor 연산이 가능하기 때문에 <PAD>에 대한 indexing도 추가한다.\n"],"metadata":{"id":"-InH6nxeQFBx"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.preprocessing import LabelEncoder\n","\n","data_df = pd.read_csv(\"/content/drive/MyDrive/여름 NLP/5일차/dataset/NER dataset.csv\", encoding=\"ISO-8859-1\")\n","data_df =data_df.loc[:100000,:]\n","print(data_df)\n","\n","dummlabel = [\"A\",\"B\",\"C\",\"D\"]\n","le = LabelEncoder()\n","le.fit(dummlabel)\n","print(le.transform(dummlabel))\n","\n","token_encoder = LabelEncoder()\n","token_encoder.fit([\"<PAD>\"] + list(data_df.loc[:,\"Word\"]))\n","\n","data_df.loc[:,\"Word\"] = token_encoder.transform(data_df.loc[:,\"Word\"])\n","token_pad_index = token_encoder.transform([\"<PAD>\"])\n","print(data_df)\n","\n","label_encoder = LabelEncoder()\n","label_encoder.fit([\"<PAD>\"]+list(data_df.loc[:,\"Tag\"]))\n","label_pad_index = label_encoder.transform([\"<PAD>\"])\n","\n","data_df.loc[:,\"Tag\"] = label_encoder.transform(data_df.loc[:,\"Tag\"])\n","print(data_df)\n","\n","data_list = []\n","label_list = []\n","for i in range(100000):\n","  if type(data_df.loc[i,\"Sentence #\"]) is str:\n","    line = []\n","    labels = []\n","  line.append(data_df.loc[i,\"Word\"])\n","  labels.append(data_df.loc[i,\"Tag\"])\n","\n","  if type(data_df.loc[i+1,\"Sentence #\"]) is str:\n","    data_list.append(line)\n","    label_list.append(labels)\n","\n","print(len(data_list))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GEETVzs0SNVX","outputId":"0a504257-9e61-4c3a-d1de-19c6f8a7ecc7","executionInfo":{"status":"ok","timestamp":1657694529274,"user_tz":-540,"elapsed":5778,"user":{"displayName":"류상연/AI·소프트웨어학부(소프트웨어전공)","userId":"00705256891800395029"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["            Sentence #           Word  POS Tag\n","0          Sentence: 1      Thousands  NNS   O\n","1                  NaN             of   IN   O\n","2                  NaN  demonstrators  NNS   O\n","3                  NaN           have  VBP   O\n","4                  NaN        marched  VBN   O\n","...                ...            ...  ...  ..\n","99996              NaN      seriously   RB   O\n","99997              NaN              .    .   O\n","99998   Sentence: 4544  Demonstrators  NNS   O\n","99999              NaN       chanting  VBG   O\n","100000             NaN              \"   ``   O\n","\n","[100001 rows x 4 columns]\n","[0 1 2 3]\n","            Sentence #  Word  POS Tag\n","0          Sentence: 1  3536  NNS   O\n","1                  NaN  8074   IN   O\n","2                  NaN  5605  NNS   O\n","3                  NaN  6826  VBP   O\n","4                  NaN  7652  VBN   O\n","...                ...   ...  ...  ..\n","99996              NaN  9522   RB   O\n","99997              NaN    15    .   O\n","99998   Sentence: 4544  1295  NNS   O\n","99999              NaN  4911  VBG   O\n","100000             NaN     1   ``   O\n","\n","[100001 rows x 4 columns]\n","            Sentence #  Word  POS  Tag\n","0          Sentence: 1  3536  NNS   17\n","1                  NaN  8074   IN   17\n","2                  NaN  5605  NNS   17\n","3                  NaN  6826  VBP   17\n","4                  NaN  7652  VBN   17\n","...                ...   ...  ...  ...\n","99996              NaN  9522   RB   17\n","99997              NaN    15    .   17\n","99998   Sentence: 4544  1295  NNS   17\n","99999              NaN  4911  VBG   17\n","100000             NaN     1   ``   17\n","\n","[100001 rows x 4 columns]\n","4543\n"]}]},{"cell_type":"markdown","source":["##Dataset  \n","전체 데이터중 4000개의 data는 train 나머지는 test로 사용한다.  \n","사전에 tokenization과 indexing된 데이터의 크기를 맞추기위해 data에 padding을 추가한다.  \n","  padding의 value는 <PAD> token의 index를 사용한다."],"metadata":{"id":"9xuA7c2zaEir"}},{"cell_type":"code","source":["from torch.utils.data import Dataset, DataLoader\n","import torch\n","\n","train_data_list = data_list[:4000]\n","test_data_list = data_list[4000:]\n","train_label_list = data_list[:4000]\n","test_label_list = data_list[4000:]\n","\n","class MyDataset(Dataset):\n","  def __init__(self,data_list,label_list) -> None:\n","      super().__init__()\n","      self.data_list = data_list\n","      self.label_list = label_list\n","\n","  def __len__(self):\n","    return len(self.data_list)\n","\n","  def __getitem__(self,index):\n","    data_pad = torch.zeros(50,dtype=torch.int64) + token_pad_index\n","    label_pad = torch.zeros(50) + label_pad_index\n","    data = torch.cat([torch.IntTensor(self.data_list[index]),data_pad])[:50]\n","    label = torch.cat([torch.IntTensor(self.label_list[index]),label_pad])[:50]\n","\n","    return data.flip(-1), label.flip(-1)\n","\n","train_dataset = MyDataset(train_data_list,train_label_list)\n","test_dataset = MyDataset(test_data_list,test_label_list)\n","\n","for i in train_dataset:\n","  print(i)\n","  break\n","\n","batch_size = 3\n","train_dataloader = DataLoader(train_dataset, batch_size = batch_size)\n","test_dataloader = DataLoader(test_dataset, batch_size = batch_size)\n","\n","for i in train_dataloader:\n","  print(i[0].shape)\n","  print(i[1].shape)\n","  break\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MWPogoV4hgTe","executionInfo":{"status":"ok","timestamp":1657694534510,"user_tz":-540,"elapsed":9,"user":{"displayName":"류상연/AI·소프트웨어학부(소프트웨어전공)","userId":"00705256891800395029"}},"outputId":"7949d006-8eca-4bcf-bdda-fd58c29978e7"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["(tensor([  487,   487,   487,   487,   487,   487,   487,   487,   487,   487,\n","          487,   487,   487,   487,   487,   487,   487,   487,   487,   487,\n","          487,   487,   487,   487,   487,   487,    15,  5371, 10223,  6574,\n","        10418,   969,  8074, 10844, 10225,  5586,  4193,  1958,  7062, 10705,\n","        10225,  8732, 10296,  2308, 10271,  7652,  6826,  5605,  8074,  3536]), tensor([    0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n","            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n","            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,    15.,\n","         5371., 10223.,  6574., 10418.,   969.,  8074., 10844., 10225.,  5586.,\n","         4193.,  1958.,  7062., 10705., 10225.,  8732., 10296.,  2308., 10271.,\n","         7652.,  6826.,  5605.,  8074.,  3536.], dtype=torch.float64))\n","torch.Size([3, 50])\n","torch.Size([3, 50])\n"]}]},{"cell_type":"markdown","source":["##Model  \n","lstm을 위한 model과 seq2seq를 이용한 모델 두가지 구성  \n","seq2seq는 encoder model과 decoder model을 따로 구성하고 encoder model의 h,c를 decoer model의 입력으로 사용한다.  \n","이후 decoder model의 hidden state 들을 예측을 위해 사용한다."],"metadata":{"id":"IH33Yo4jqT19"}},{"cell_type":"code","source":["from torch import nn\n","\n","class myModel(nn.Module):\n","  def __init__(self) -> None:\n","      super().__init__()\n","      self.emb = nn.Embedding(35179, 128)\n","      self.lstm = nn.LSTM(128,128, batch_first=True)\n","      self.ln1 = nn.Linear(128,20)\n","      self.relu = nn.ReLU()\n","  def forward(self,x):\n","    x = self.emb(x)\n","    x, (h,c) = self.lstm(x)\n","    print(x.shape)\n","    x = self.ln1(x)\n","    x = self.relu(x)\n","\n","    return x\n","\n","\n","model = myModel()\n","\n","for i in train_dataloader:\n","  print(i[0].shape)\n","  print(model(i[0]).shape)\n","  break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TkwRy4S_mVw8","executionInfo":{"status":"ok","timestamp":1657694638865,"user_tz":-540,"elapsed":8,"user":{"displayName":"류상연/AI·소프트웨어학부(소프트웨어전공)","userId":"00705256891800395029"}},"outputId":"0ec7f930-bd70-4aef-96b1-5f6c73d91edc"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3, 50])\n","torch.Size([3, 50, 128])\n","torch.Size([3, 50, 20])\n"]}]},{"cell_type":"code","source":["from torch import nn\n","\n","class myModel(nn.Module):\n","  def __init__(self) -> None:\n","      super().__init__()\n","\n","      self.emb = nn.Embedding(35179, 128)\n","      self.lstm = nn.LSTM(128,128, batch_first=True, bidirectional=True)\n","      self.ln1 = nn.Linear(128,20)\n","\n","  def forward(self, x):\n","      x = self.emb(x)\n","      x, (h,c) = self.lstm(x)\n","      print(x.shape)\n","      x = self.ln1(x)\n","\n","      return x\n","\n","model = myModel()\n","\n","for i in train_dataloader:\n","  print(i[0].shape)\n","  print(model(i[0]).shape)\n","  break"],"metadata":{"id":"yqgzfvsAsTKv","colab":{"base_uri":"https://localhost:8080/","height":392},"executionInfo":{"status":"error","timestamp":1657694626038,"user_tz":-540,"elapsed":10,"user":{"displayName":"류상연/AI·소프트웨어학부(소프트웨어전공)","userId":"00705256891800395029"}},"outputId":"45d9b955-b400-4453-e4fb-61aa5514360b"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3, 50])\n","torch.Size([3, 50, 256])\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-29deaaa2d31a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m   \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-29deaaa2d31a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m       \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (150x256 and 128x20)"]}]},{"cell_type":"code","source":["from torch import nn\n","\n","class myModel(nn.Module):\n","  def __init__(self) -> None:\n","      super().__init__()\n","\n","      self.en_emb = nn.Embedding(50000, 128)\n","      self.en_lstm = nn.LSTM(128,128, batch_first=True)\n","      self.en_ln1 = nn.Linear(128,20)\n","\n","      \n","      self.de_emb = nn.Embedding(50000, 128)\n","      self.de_lstm = nn.LSTM(128,128, batch_first=True)\n","      self.de_ln1 = nn.Linear(128,20)\n","\n","  def forward(self, x):\n","      x = self.en_emb(x)\n","      x, (h,c) = self.en_lstm(x)\n","\n","      x, _ = self.de_lstm(x, (h,c))\n","      x = self.de_ln1(x)\n","\n","      return x\n","\n","model = myModel()\n","\n","for i in train_dataloader:\n","  print(i[0].shape)\n","  print(model(i[0]).shape)\n","  break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0Jr9S--alME2","executionInfo":{"status":"ok","timestamp":1657694122285,"user_tz":-540,"elapsed":296,"user":{"displayName":"류상연/AI·소프트웨어학부(소프트웨어전공)","userId":"00705256891800395029"}},"outputId":"6b73d65a-28b6-4a8d-9738-1554c01b1970"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3, 50])\n","torch.Size([3, 50, 20])\n"]}]},{"cell_type":"markdown","source":["optimization"],"metadata":{"id":"xh6z1SkYqmJa"}},{"cell_type":"code","source":["from torch.optim import Adam\n","from torch.nn import CrossEntropyLoss\n","\n","model = myModel()\n","model.cuda()\n","\n","#학습을 위한 optimizer와 loss function 설정\n","optimizer = Adam(model.parameters(), lr=0.0001)\n","lf = CrossEntropyLoss().cuda()\n","\n","\n","#100번의 에폭을 실행\n","for e in range(100):\n","  print(\"\\n\\nepoch \", e)\n","  epoch_loss = 0\n","  train_correct = 0 \n","  \n","  #선언한 모델 오브젝트를 학습가능한 상태로 변경\n","  model.train()\n","\n","  #모든 학습데이터에 대해서 학습\n","  for i in train_dataloader:\n","    #매 배치에 대한 gradient계산 이전에 optimizer에 저장된 이전 batch에 gradient를 삭제(초기화)\n","    optimizer.zero_grad()\n","    data = i[0]\n","    data = data.cuda()\n","    target = i[1]\n","    target = target.reshape(-1).cuda()\n","    #print(target.shape)\n","\n","    #결과 도출 및 정답수 연산\n","    output = model(data).reshape(-1,20)\n","    #print(output.shape)\n","    pred_label = torch.argmax(output, dim=-1)\n","    #print(pred_label.shape)\n","    #print(target.shape)\n","    train_correct += sum(pred_label == target)\n","    #print(sum(pred_label == target))\n","    #print(pred_label)\n","    #print(target)\n","    # print(data)\n","\n","    #loss연산\n","    loss = lf(output, target.long())\n","    #print(loss)\n","\n","    #loss backpropagation\n","    loss.backward()\n","\n","    #gradient update\n","    optimizer.step()\n","\n","    epoch_loss += loss.item()\n","  \n","  print(train_correct)\n","  print(\"train loss\", epoch_loss/len(train_dataloader))\n","  print(\"train acc\", train_correct/len(train_dataset))\n","\n","  #model이 학습되지 않는 상태로 변경\n","  model.eval()\n","  test_loss = 0\n","  test_correct = 0 \n","\n","  #gradient를 계산하지 않도록 하여 cost낭비 방지\n","  with torch.no_grad():\n","    #모든 test dataset에 대해서 결과연산\n","    for i in test_dataloader:\n","      test_data = i[0]\n","      test_target = i[1]\n","      test_data = test_data.cuda()\n","      test_target = test_target.reshape(-1).cuda()\n","      #print(test_target.shape)\n","\n","      test_output = model(test_data).reshape(-1,20)\n","      #print(test_output.shape)\n","\n","      tloss = lf(test_output, test_target.long())\n","      test_pred_label = torch.argmax(test_output, dim=-1)\n","      #print(test_pred_label.shape)\n","      test_correct += sum(test_pred_label == test_target)\n","      test_loss += tloss.item()\n","\n","  print(\"test loss\", test_loss/len(test_dataloader))\n","  print(\"test acc\", test_correct/len(test_dataset))\n","    \n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":409},"id":"9p02322wqs1k","outputId":"ddceeb2c-f08a-49f4-cee2-23d87ce35bb9","executionInfo":{"status":"error","timestamp":1657694149144,"user_tz":-540,"elapsed":298,"user":{"displayName":"류상연/AI·소프트웨어학부(소프트웨어전공)","userId":"00705256891800395029"}}},"execution_count":25,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-036d20306efe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#학습을 위한 optimizer와 loss function 설정\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    687\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m         \"\"\"\n\u001b[0;32m--> 689\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mipu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    687\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m         \"\"\"\n\u001b[0;32m--> 689\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mipu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."]}]}]}