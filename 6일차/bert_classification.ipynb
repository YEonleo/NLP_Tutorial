{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7684,"status":"ok","timestamp":1658304785316,"user":{"displayName":"류상연/AI·소프트웨어학부(소프트웨어전공)","userId":"00705256891800395029"},"user_tz":-540},"id":"ZhNm9JaF1uct","outputId":"19176a3e-5212-46a5-f759-64c1bce44653"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4135,"status":"ok","timestamp":1658304789443,"user":{"displayName":"류상연/AI·소프트웨어학부(소프트웨어전공)","userId":"00705256891800395029"},"user_tz":-540},"id":"bVmB8Ln41zLT","outputId":"0358d14a-da77-4bd3-f660-c2d5a87e7d8e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.20.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.8.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n"]}],"source":["!pip install transformers"]},{"cell_type":"markdown","metadata":{"id":"68VPO5kS10dl"},"source":["#construct dataset  \n","dataset구성\n","spam dataset을 하나씩 반환하며  \n","BertTokenzier를 통해 tokenziation후 input_ids, token_type_ids, attention mask를 tensor로 바꿔서 return  \n","또한 label이 spam일때는 1 ham일때는 0을 함께 return  "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13505,"status":"ok","timestamp":1658304802936,"user":{"displayName":"류상연/AI·소프트웨어학부(소프트웨어전공)","userId":"00705256891800395029"},"user_tz":-540},"id":"-NI5SX_Z2Hfj","outputId":"1907898c-f858-4cf4-fa7a-4d212bded03c"},"outputs":[{"output_type":"stream","name":"stdout","text":["        v1                                                 v2 Unnamed: 2  \\\n","0      ham  Go until jurong point, crazy.. Available only ...        NaN   \n","1      ham                      Ok lar... Joking wif u oni...        NaN   \n","2     spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n","3      ham  U dun say so early hor... U c already then say...        NaN   \n","4      ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n","...    ...                                                ...        ...   \n","5567  spam  This is the 2nd time we have tried 2 contact u...        NaN   \n","5568   ham              Will Ì_ b going to esplanade fr home?        NaN   \n","5569   ham  Pity, * was in mood for that. So...any other s...        NaN   \n","5570   ham  The guy did some bitching but I acted like i'd...        NaN   \n","5571   ham                         Rofl. Its true to its name        NaN   \n","\n","     Unnamed: 3 Unnamed: 4  \n","0           NaN        NaN  \n","1           NaN        NaN  \n","2           NaN        NaN  \n","3           NaN        NaN  \n","4           NaN        NaN  \n","...         ...        ...  \n","5567        NaN        NaN  \n","5568        NaN        NaN  \n","5569        NaN        NaN  \n","5570        NaN        NaN  \n","5571        NaN        NaN  \n","\n","[5572 rows x 5 columns]\n","Index(['v1', 'v2'], dtype='object')\n","torch.Size([3, 512])\n","torch.Size([3, 512])\n","torch.Size([3, 512])\n","tensor([0, 0, 1])\n","[tensor([[ 101, 2175, 2127,  ...,    0,    0,    0],\n","        [ 101, 7929, 2474,  ...,    0,    0,    0],\n","        [ 101, 2489, 4443,  ...,    0,    0,    0]], dtype=torch.int32), tensor([[0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.int32), tensor([[1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0]], dtype=torch.int32), tensor([0, 0, 1])]\n"]}],"source":["import torch\n","import pandas as pd\n","\n","from transformers import AutoTokenizer,BertTokenizer\n","from torchtext.data.utils import get_tokenizer\n","from torchtext.vocab import build_vocab_from_iterator\n","from torch.utils.data import Dataset, DataLoader\n","\n","\n","data_path = \"/content/drive/MyDrive/여름 NLP/6일차/dataset/spam.csv\"\n","data_df = pd.read_csv(data_path,encoding=\"ISO-8859-1\")\n","\n","print(data_df)\n","\n","data_df = data_df.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis= 1)\n","\n","print(data_df.columns)\n","\n","#index 0부터 시작하기위해 사용\n","train_df = data_df.loc[:4000,:].reset_index()\n","test_df = data_df.loc[4000:,:].reset_index()\n","\n","\n","#토큰화\n","tokenizer = get_tokenizer(\"basic_english\")\n","\n","ex_text = train_df.loc[0,\"v2\"]\n","\n","#토큰화한걸 숫자로 변경\n","vocab = build_vocab_from_iterator(list(map(tokenizer,data_df.loc[:,\"v2\"])))\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n","\n","class MyDataset(Dataset):\n","  def __init__(self,df,tokenizer) -> None:\n","      super().__init__()\n","      self.df = df\n","      self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","  \n","  def __len__(self):\n","    return len(self.df)\n","  \n","  def __getitem__(self,index):\n","    data = self.df.loc[index,\"v2\"]\n","    data = self.tokenizer(data, max_length=512,padding='max_length')\n","    \n","    if self.df.loc[index,\"v1\"]==\"spam\":\n","      label = 1\n","    elif self.df.loc[index,\"v1\"]==\"ham\":\n","      label = 0\n","\n","    input_ids = torch.IntTensor(data[\"input_ids\"])\n","    token_type_ids = torch.IntTensor(data[\"token_type_ids\"])\n","    attention_mask = torch.IntTensor(data[\"attention_mask\"])\n","\n","    return input_ids, token_type_ids, attention_mask,label\n","\n","train_dataset = MyDataset(train_df, tokenizer)\n","test_dataset = MyDataset(test_df, tokenizer)\n","\n","\n","batch_size = 3\n","train_dataloader = DataLoader(train_dataset,batch_size = batch_size)\n","test_dataloader = DataLoader(test_dataset,batch_size = batch_size)\n","\n","for i in train_dataloader:\n","  input_ids = i[0]\n","  token_type_ids = i[1]\n","  attention_mask = i[2]\n","  label = i[3]\n","  print(input_ids.shape)\n","  print(token_type_ids.shape)\n","  print(attention_mask.shape)\n","  print(label)\n","  print(i)\n","  break\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"a_1-izAu2H6G"},"source":["##Model\n","BertModel을 이용해서 구현  \n","BertModel의 cls token에 대한 output인 pooler_output에 linear를 통해 2차원 벡터로 변환  \n","BertModel: https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertModel"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7976,"status":"ok","timestamp":1658305735383,"user":{"displayName":"류상연/AI·소프트웨어학부(소프트웨어전공)","userId":"00705256891800395029"},"user_tz":-540},"id":"St-O1cxh2ibK","outputId":"dc4d0b7a-c3ca-4454-f6a4-a353fe43f737"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 512, 768])\n","torch.Size([3, 512, 1])\n","torch.Size([3])\n"]}],"source":["from torch import nn\n","from transformers import BertModel\n","\n","\n","class MyModel(nn.Module):\n","  def __init__(self) -> None:\n","      super().__init__()\n","      self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n","      self.ln1 = nn.Linear(768,1)\n","\n","  def forward(self,input_ids, token_type_ids, attention_mask, label):\n","    out = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n","    out = out[\"last_hidden_state\"]\n","    print(out.shape)\n","    out = self.ln1(out)\n","    print(out.shape)\n","    print(label.shape)\n","\n","\n","    return out.squeeze()\n","model = MyModel()\n","\n","for i in train_dataloader:\n","  input_ids = i[0]\n","  token_type_ids = i[1]\n","  attention_mask = i[2]\n","  label = i[3]\n","  out = model(input_ids, token_type_ids, attention_mask,label )\n","  #print(out[0].size(),out[1].size())\n","  #print(i[3])\n","  break"]},{"cell_type":"markdown","metadata":{"id":"bQN7lqE22eMu"},"source":["##train\n","cross entropy loss와 Adam을 통해 train"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"GZm0ceuw2iE9","outputId":"d276dacc-953a-49dd-d904-7f2c1a7bb0db","executionInfo":{"status":"error","timestamp":1658305614966,"user_tz":-540,"elapsed":12911,"user":{"displayName":"류상연/AI·소프트웨어학부(소프트웨어전공)","userId":"00705256891800395029"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n"," epoch 0\n","torch.Size([3])\n","torch.Size([3, 512, 768])\n","torch.Size([3, 512, 1])\n","torch.Size([3, 512])\n","torch.Size([3])\n","torch.Size([3, 512, 768])\n","torch.Size([3, 512, 1])\n","torch.Size([3, 512])\n","torch.Size([3])\n","torch.Size([3, 512, 768])\n","torch.Size([3, 512, 1])\n","torch.Size([3, 512])\n","torch.Size([3])\n","torch.Size([3, 512, 768])\n","torch.Size([3, 512, 1])\n","torch.Size([3, 512])\n","torch.Size([3])\n","torch.Size([3, 512, 768])\n","torch.Size([3, 512, 1])\n","torch.Size([3, 512])\n","torch.Size([3])\n","torch.Size([3, 512, 768])\n","torch.Size([3, 512, 1])\n","torch.Size([3, 512])\n","torch.Size([3])\n","torch.Size([3, 512, 768])\n","torch.Size([3, 512, 1])\n","torch.Size([3, 512])\n","torch.Size([3])\n","torch.Size([3, 512, 768])\n","torch.Size([3, 512, 1])\n","torch.Size([3, 512])\n","torch.Size([3])\n","torch.Size([3, 512, 768])\n","torch.Size([3, 512, 1])\n","torch.Size([3, 512])\n","torch.Size([3])\n","torch.Size([3, 512, 768])\n","torch.Size([3, 512, 1])\n","torch.Size([3, 512])\n","torch.Size([3])\n","torch.Size([3, 512, 768])\n","torch.Size([3, 512, 1])\n","torch.Size([3, 512])\n","torch.Size([3])\n","torch.Size([3, 512, 768])\n","torch.Size([3, 512, 1])\n","torch.Size([3, 512])\n","torch.Size([3])\n","torch.Size([3, 512, 768])\n","torch.Size([3, 512, 1])\n","torch.Size([3, 512])\n","torch.Size([3])\n","torch.Size([3, 512, 768])\n","torch.Size([3, 512, 1])\n","torch.Size([3, 512])\n","torch.Size([3])\n","torch.Size([3, 512, 768])\n","torch.Size([3, 512, 1])\n","torch.Size([3, 512])\n","torch.Size([3])\n","torch.Size([3, 512, 768])\n","torch.Size([3, 512, 1])\n","torch.Size([3, 512])\n","torch.Size([3])\n","torch.Size([3, 512, 768])\n","torch.Size([3, 512, 1])\n","torch.Size([3, 512])\n","torch.Size([3])\n","torch.Size([3, 512, 768])\n","torch.Size([3, 512, 1])\n","torch.Size([3, 512])\n","torch.Size([3])\n","torch.Size([3, 512, 768])\n","torch.Size([3, 512, 1])\n","torch.Size([3, 512])\n","torch.Size([3])\n","torch.Size([3, 512, 768])\n","torch.Size([3, 512, 1])\n","torch.Size([3, 512])\n","torch.Size([3])\n","torch.Size([3, 512, 768])\n","torch.Size([3, 512, 1])\n","torch.Size([3, 512])\n","torch.Size([3])\n","torch.Size([3, 512, 768])\n","torch.Size([3, 512, 1])\n","torch.Size([3, 512])\n","torch.Size([3])\n","torch.Size([3, 512, 768])\n","torch.Size([3, 512, 1])\n","torch.Size([3, 512])\n","torch.Size([3])\n","torch.Size([3, 512, 768])\n","torch.Size([3, 512, 1])\n","torch.Size([3, 512])\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-8c67d838b5c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    169\u001b[0m                  \u001b[0mmaximize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'maximize'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                  \u001b[0mforeach\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'foreach'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m                  capturable=group['capturable'])\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    224\u001b[0m          \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m          \u001b[0mmaximize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m          capturable=capturable)\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["from torch.optim import Adam\n","\n","model = MyModel()\n","model.cuda()\n","\n","optimizer = Adam(model.parameters(), lr=0.0001) \n","lf = nn.CrossEntropyLoss()\n","\n","for e in range(30):\n","  print(\"\\n\\n epoch\", e)\n","  model.train()\n","  train_ac = 0\n","  epoch_loss = 0\n","\n","  for i in train_dataloader:\n","    optimizer.zero_grad()\n","\n","    input_ids = i[0]\n","    input_ids = input_ids.cuda()\n","    token_type_ids = i[1]\n","    token_type_ids = token_type_ids.cuda()\n","    attention_mask = i[2]\n","    attention_mask = attention_mask.cuda()\n","    target = i[3]\n","    target = target.cuda()\n","    print(target.shape)\n","\n","    output = model(input_ids,token_type_ids,attention_mask,target)\n","    print(output.shape)\n","\n","    pred_label = torch.argmax(output, dim=-1)\n","    target = target.reshape(-1)\n","\n","    train_acc = sum(pred_label == target.reshape(-1))\n","    train_ac += train_acc\n","    \n","    loss = lf(output,target)\n","    \n","\n","    loss.backward()\n","    optimizer.step()\n","\n","    epoch_loss += loss.item()\n","  print(train_acc)\n","  print(\"train loss\", epoch_loss/len(train_dataloader))\n","  print(\"train acc\", train_ac/len(train_dataset))\n","\n","  model.eval()\n","  test_loss = 0\n","  test_ac = 0 \n","\n","  with torch.no_grad():\n","    for i in test_dataloader:\n","      input_ids = i[0]\n","      input_ids = input_ids.cuda()\n","      token_type_ids = i[1]\n","      token_type_ids = token_type_ids.cuda()\n","      attention_mask = i[2]\n","      attention_mask = attention_mask.cuda()\n","      target = i[3]\n","      target = target.cuda()\n","\n","      output = model(input_ids,token_type_ids,attention_mask)\n","\n","      test_pred_label = torch.argmax(output, dim=-1)\n","      target = target.reshape(-1)\n","\n","      test_acc = sum(test_pred_label == target.reshape(-1))\n","      test_ac += test_acc\n","    \n","      loss = lf(output,target)\n","\n","      test_loss += loss.item()\n","\n","  print(\"test loss\", test_loss/len(test_dataloader))\n","  print(\"test acc\", train_ac/len(test_dataset))\n","\n","\n","    "]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}