{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Spam SNS classification.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMtKZRDDGAnOokluMra6MIi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4Vy_G2wCjua9","executionInfo":{"status":"ok","timestamp":1657503311747,"user_tz":-540,"elapsed":25545,"user":{"displayName":"김균엽","userId":"17069744786161993916"}},"outputId":"78b16337-7440-4b47-ece7-b94e1515f8e7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X2Eq1SjUWGER","executionInfo":{"status":"ok","timestamp":1657480940001,"user_tz":-540,"elapsed":12742,"user":{"displayName":"김균엽","userId":"17069744786161993916"}},"outputId":"730b10db-352f-40a1-af9c-392d2b81542d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n","\u001b[K     |████████████████████████████████| 4.4 MB 19.0 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 23.9 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 38.2 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n","\u001b[K     |████████████████████████████████| 101 kB 5.2 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.8.1 pyyaml-6.0 tokenizers-0.12.1 transformers-4.20.1\n"]}]},{"cell_type":"markdown","source":["# Spam SNS classification"],"metadata":{"id":"RjVOZjEU2pan"}},{"cell_type":"markdown","source":["각 문장이 spam 메일인지 아닌지를 판별하는 문장 분류 태스크  \n","dataset: https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset  \n","4000개까지 train으로 사용  \n","\n","Dataset 구축  \n","dataset에서는 __ getitem __을 통해 한개의 데이터를 모델의 입력형태에 맞추어 반환한다.  \n","이번 코드에서는 자연어 문장을 모델에 입력하기 위해 tokenization과 vocab dictionary에 따른 index로의 변환을 진행한다.  \n","또한 label의 ham/spam에 따라 0/1을 label로 변환한다.  \n","\n","주요 class, method:  \n","torchtext.data.utils.get_tokenizer: torchtext에서 제공하는 tokenizer 문법에 따른 tokenization을 수행하는 class 반환  \n","torchtext.vocab.build_vocab_from_iterator:  내 학습데이터에 대한 모든 단어를 입력하면 각 단어에 한개씩 index를 부여한 vocab dictionary 반환  \n","**huggingface.tokenizer:** 사전에 학습된 tokenization을 불러오는 huggingface class  \n","https://huggingface.co/docs/transformers/main_classes/tokenizer\n"],"metadata":{"id":"Hu1IWE_L2xM3"}},{"cell_type":"code","source":["import pandas as pd\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","\n","from transformers import AutoTokenizer,BertTokenizer\n","from torchtext.data.utils import get_tokenizer\n","from torchtext.vocab import build_vocab_from_iterator\n","\n","#dataset load\n","data_path = \"/content/drive/MyDrive/랩세미나(new)/tutorial/open_tutorial_DL_NLP/4일차/dataset/spam.csv\"\n","data_df = pd.read_csv(data_path, encoding = \"ISO-8859-1\")\n","print(data_df)\n","print(data_df.columns)\n","\n","#train test split\n","data_df = data_df.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis=1)\n","train_df = data_df.loc[:200,:].reset_index()\n","test_df = data_df.loc[4000:,:].reset_index()\n","print(train_df)\n","print(train_df.columns)\n","\n","#tokenizer load\n","tokenizer=get_tokenizer(\"basic_english\")\n","d = data_df.loc[0:3,\"v2\"]\n","print(d)\n","print(tokenizer(data_df.loc[0,\"v2\"]))\n","\n","#build vocab dictionary\n","vocab = build_vocab_from_iterator(list(map(tokenizer,data_df.loc[:,\"v2\"])))\n","print(vocab(tokenizer(data_df.loc[0,\"v2\"])))\n","\n","#huggingface tokenizer load\n","tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n","print(tokenizer(data_df.loc[0,\"v2\"], max_length=100, padding='max_length', truncation=True))\n","\n","class myDataset(Dataset):\n","  def __init__(self, df, tokenizer, vocab) -> None:\n","      super().__init__()\n","      self.df = df\n","      self.tokenizer = tokenizer\n","      self.vocab = vocab\n","\n","  def __len__(self):\n","      return len(self.df)\n","  \n","  def __getitem__(self, index):\n","      data = self.df.loc[index, \"v2\"]\n","      target = self.df.loc[index, \"v1\"]\n","      \n","      #data tokenization\n","      data = self.tokenizer(data, max_length=100,padding='max_length', truncation=True)[\"input_ids\"]\n","\n","      #labeling\n","      if target == \"ham\":\n","          label = 0\n","      elif target == \"spam\":\n","          label = 1\n","\n","      return data, label\n","\n","\n","train_dataset = myDataset(train_df, tokenizer, vocab)\n","test_dataset = myDataset(test_df, tokenizer, vocab)\n","batch_size = 100\n","train_dataloader = DataLoader(train_dataset, batch_size = batch_size)\n","test_dataloader = DataLoader(test_dataset, batch_size = batch_size)\n","\n","for i in train_dataset:\n","  print(i)\n","  break\n","  \n","for i in train_dataloader:\n","  print(i)\n","  break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":136},"id":"8kWdTTr8XWal","executionInfo":{"status":"error","timestamp":1657503275568,"user_tz":-540,"elapsed":5,"user":{"displayName":"김균엽","userId":"17069744786161993916"}},"outputId":"bcc42685-fce4-4982-e1c0-745a2664634f"},"execution_count":null,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-9ab2e93a0ab6>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    https://huggingface.co/docs/transformers/main_classes/tokenizerimport pandas as pd\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}]},{"cell_type":"markdown","source":["model 선언  \n","문장을 RNN/LSTM을 통해 classification하는 model 선언\n","1. token의 index를 입력으로 받고 word embedding을 결과로 반환하는 nn.Embedding 사용  \n","2. LSTM을 통해 모든 token을 입력  \n","3. many-to-one구조를 가지기 때문에 LSTM의 결과중 마지막 cell에 대한 결과만을 사용하여 nn.linear를통해 classification\n","\n","입력으로는 문장을 tokenziation과 indexing한게 입력으로 들어오기 때문에(batch, sequance length)형태의 입력  \n","이후 nn.Embedding을 거치면서 각 단어의 벡터가 생성되기 때문에(batch, sequance length, hidden size)의 형태 사용\n","\n","주요 obejct:  \n","**nn.Embedding:**https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html  \n","**nn.LSTM:**https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html  \n","**nn.RNN:**https://pytorch.org/docs/stable/generated/torch.nn.RNN.html"],"metadata":{"id":"xfKV9_575XYT"}},{"cell_type":"code","source":["from torch import nn\n","\n","class myModel(nn.Module):\n","    def __init__(self) -> None:\n","        super().__init__()\n","        self.emb = nn.Embedding(50000,128)\n","        self.lstm = nn.LSTM(128, 128)\n","        self.linaer = nn.Linear(128, 2)\n","\n","    def forward(self, x):\n","\n","        x = self.emb(x)\n","        x, _ = self.lstm(x)\n","        \n","        #마지막 time(seq length)에 대한 결과만을 사용하여 classification\n","        x = self.linaer(x[:,-1,:])\n","\n","        return x\n","\n","model = myModel()\n","\n","for i in train_dataloader:\n","  data = i[0]\n","  label = i[1]\n","  data = model(data)\n","  print(data)\n","  break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ss1jJefWEw6X","executionInfo":{"status":"ok","timestamp":1657481438521,"user_tz":-540,"elapsed":312,"user":{"displayName":"김균엽","userId":"17069744786161993916"}},"outputId":"a8a04748-4bec-4bd8-dc7b-febbfeda4b98"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[-0.1231,  0.0153],\n","        [-0.1830, -0.0108],\n","        [-0.2121, -0.0281],\n","        [-0.2269, -0.0392],\n","        [-0.2350, -0.0463],\n","        [-0.2394, -0.0507],\n","        [-0.2420, -0.0536],\n","        [-0.2433, -0.0554],\n","        [-0.2441, -0.0566],\n","        [-0.2444, -0.0573],\n","        [-0.2445, -0.0578],\n","        [-0.2445, -0.0581],\n","        [-0.2445, -0.0582],\n","        [-0.2443, -0.0583],\n","        [-0.2442, -0.0584],\n","        [-0.2441, -0.0584],\n","        [-0.2440, -0.0584],\n","        [-0.2439, -0.0584],\n","        [-0.2438, -0.0583],\n","        [-0.2437, -0.0583],\n","        [-0.2436, -0.0583],\n","        [-0.2435, -0.0583],\n","        [-0.2435, -0.0582],\n","        [-0.2434, -0.0582],\n","        [-0.2434, -0.0582],\n","        [-0.2434, -0.0582],\n","        [-0.2433, -0.0582],\n","        [-0.2433, -0.0582],\n","        [-0.2433, -0.0581],\n","        [-0.2432, -0.0581],\n","        [-0.2432, -0.0581],\n","        [-0.2432, -0.0581],\n","        [-0.2432, -0.0581],\n","        [-0.2432, -0.0581],\n","        [-0.2432, -0.0581],\n","        [-0.2432, -0.0581],\n","        [-0.2432, -0.0581],\n","        [-0.2432, -0.0581],\n","        [-0.2432, -0.0581],\n","        [-0.2431, -0.0581],\n","        [-0.2431, -0.0581],\n","        [-0.2431, -0.0581],\n","        [-0.2431, -0.0580],\n","        [-0.2431, -0.0580],\n","        [-0.2431, -0.0580],\n","        [-0.2431, -0.0580],\n","        [-0.2431, -0.0580],\n","        [-0.2431, -0.0580],\n","        [-0.2431, -0.0580],\n","        [-0.2431, -0.0580],\n","        [-0.2431, -0.0580],\n","        [-0.2431, -0.0580],\n","        [-0.2431, -0.0580],\n","        [-0.2431, -0.0580],\n","        [-0.2431, -0.0580],\n","        [-0.2431, -0.0580],\n","        [-0.2431, -0.0580],\n","        [-0.2431, -0.0580],\n","        [-0.2431, -0.0580],\n","        [-0.2431, -0.0580],\n","        [-0.2431, -0.0580],\n","        [-0.2431, -0.0580],\n","        [-0.2431, -0.0580],\n","        [-0.2431, -0.0580],\n","        [-0.2431, -0.0580],\n","        [-0.2431, -0.0580],\n","        [-0.2431, -0.0580],\n","        [-0.2431, -0.0580],\n","        [-0.2431, -0.0580],\n","        [-0.2431, -0.0580],\n","        [-0.2431, -0.0580],\n","        [-0.2431, -0.0580],\n","        [-0.2431, -0.0580],\n","        [-0.2431, -0.0580],\n","        [-0.2431, -0.0580],\n","        [-0.2431, -0.0580],\n","        [-0.2431, -0.0580],\n","        [-0.2431, -0.0580],\n","        [-0.2431, -0.0580],\n","        [-0.2431, -0.0580],\n","        [-0.2431, -0.0580],\n","        [-0.2431, -0.0580],\n","        [-0.2431, -0.0580],\n","        [-0.2431, -0.0580],\n","        [-0.2431, -0.0580],\n","        [-0.2431, -0.0580],\n","        [-0.2431, -0.0580],\n","        [-0.2431, -0.0580],\n","        [-0.2431, -0.0580],\n","        [-0.2431, -0.0580],\n","        [-0.2431, -0.0580],\n","        [-0.2431, -0.0580],\n","        [-0.2431, -0.0580],\n","        [-0.2431, -0.0580],\n","        [-0.2431, -0.0580],\n","        [-0.2431, -0.0580],\n","        [-0.2431, -0.0580],\n","        [-0.2431, -0.0580],\n","        [-0.2431, -0.0580],\n","        [-0.2431, -0.0580]], grad_fn=<AddmmBackward0>)\n"]}]},{"cell_type":"markdown","source":["model train"],"metadata":{"id":"sg8IRfDM7Z_3"}},{"cell_type":"code","source":["from torch.optim import Adam\n","from torch.nn import CrossEntropyLoss\n","\n","model = myModel()\n","model.cuda()\n","\n","#학습을 위한 optimizer와 loss function 설정\n","optimizer = Adam(model.parameters(), lr=0.001)\n","lf = CrossEntropyLoss().cuda()\n","\n","\n","#100번의 에폭을 실행\n","for e in range(100):\n","  print(\"\\n\\nepoch \", e)\n","  epoch_loss = 0\n","  train_correct = 0 \n","  \n","  #선언한 모델 오브젝트를 학습가능한 상태로 변경\n","  model.train()\n","\n","  #모든 학습데이터에 대해서 학습\n","  for i in train_dataloader:\n","    #매 배치에 대한 gradient계산 이전에 optimizer에 저장된 이전 batch에 gradient를 삭제(초기화)\n","    optimizer.zero_grad()\n","    data = i[0]\n","    data = data.cuda()\n","    target = i[1]\n","    target = target.cuda()\n","\n","    #결과 도출 및 정답수 연산\n","    output = model(data)\n","    pred_label = torch.argmax(output, dim=-1)\n","    train_correct += sum(pred_label == target.reshape(-1))\n","\n","    target = target.reshape(-1)\n","    #loss연산\n","    loss = lf(output, target)\n","    #print(loss)\n","\n","    #loss backpropagation\n","    loss.backward()\n","\n","    #gradient update\n","    optimizer.step()\n","\n","    epoch_loss += loss.item()\n","  \n","  print(train_correct)\n","  print(\"train loss\", epoch_loss/len(train_dataloader))\n","  print(\"train acc\", train_correct/len(train_dataset))\n","\n","  #model이 학습되지 않는 상태로 변경\n","  model.eval()\n","  test_loss = 0\n","  test_correct = 0 \n","\n","  #gradient를 계산하지 않도록 하여 cost낭비 방지\n","  with torch.no_grad():\n","    #모든 test dataset에 대해서 결과연산\n","    for i in test_dataloader:\n","      data = i[0]\n","      target = i[1]\n","      data = data.cuda()\n","      target = target.cuda()\n","\n","      output = model(data)\n","\n","      loss = lf(output, target.reshape(-1))\n","      pred_label = torch.argmax(output, dim=-1)\n","      test_correct += sum(pred_label == target.reshape(-1))\n","      test_loss += loss.item()\n","\n","  print(\"test loss\", test_loss/len(test_dataloader))\n","  print(\"test acc\", test_correct/len(test_dataset))\n","    \n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zmVPBc1AN4j_","executionInfo":{"status":"ok","timestamp":1657481571738,"user_tz":-540,"elapsed":97980,"user":{"displayName":"김균엽","userId":"17069744786161993916"}},"outputId":"380d6f0d-6753-473f-a465-038d6354d611"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","epoch  0\n","tensor(101, device='cuda:0')\n","train loss 0.6825631856918335\n","train acc tensor(0.5025, device='cuda:0')\n","test loss 0.41661655716598034\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  1\n","tensor(168, device='cuda:0')\n","train loss 0.4302452703317006\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.40795064345002174\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  2\n","tensor(168, device='cuda:0')\n","train loss 0.41582388679186505\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.4223208408802748\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  3\n","tensor(168, device='cuda:0')\n","train loss 0.410207857688268\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.41877398639917374\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  4\n","tensor(168, device='cuda:0')\n","train loss 0.3971082667509715\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.4098412301391363\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  5\n","tensor(168, device='cuda:0')\n","train loss 0.3834702620903651\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.4020993858575821\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  6\n","tensor(168, device='cuda:0')\n","train loss 0.3722698390483856\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.39752085506916046\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  7\n","tensor(168, device='cuda:0')\n","train loss 0.3638334423303604\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.39576661959290504\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  8\n","tensor(168, device='cuda:0')\n","train loss 0.357279176513354\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.39574589021503925\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  9\n","tensor(168, device='cuda:0')\n","train loss 0.3515831579764684\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.39637753553688526\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  10\n","tensor(168, device='cuda:0')\n","train loss 0.34609536329905194\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.3969103470444679\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  11\n","tensor(168, device='cuda:0')\n","train loss 0.34063269942998886\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.3971109166741371\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  12\n","tensor(168, device='cuda:0')\n","train loss 0.33539271851380664\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.39711693301796913\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  13\n","tensor(168, device='cuda:0')\n","train loss 0.3307066063086192\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.39717942476272583\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  14\n","tensor(168, device='cuda:0')\n","train loss 0.32665935655434925\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.3974226228892803\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  15\n","tensor(168, device='cuda:0')\n","train loss 0.3231167495250702\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.397808987647295\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  16\n","tensor(168, device='cuda:0')\n","train loss 0.3200894792874654\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.39835770800709724\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  17\n","tensor(168, device='cuda:0')\n","train loss 0.31744517510135967\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.3990748841315508\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  18\n","tensor(168, device='cuda:0')\n","train loss 0.31511591002345085\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.3999218214303255\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  19\n","tensor(168, device='cuda:0')\n","train loss 0.31311625987291336\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.4007960893213749\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  20\n","tensor(168, device='cuda:0')\n","train loss 0.31142385428150493\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.4015454351902008\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  21\n","tensor(168, device='cuda:0')\n","train loss 0.30998706817626953\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.4020560886710882\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  22\n","tensor(168, device='cuda:0')\n","train loss 0.3087551264713208\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.40232851915061474\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  23\n","tensor(168, device='cuda:0')\n","train loss 0.3076966522882382\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.4024672191590071\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  24\n","tensor(168, device='cuda:0')\n","train loss 0.3067903301368157\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.4026035722345114\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  25\n","tensor(168, device='cuda:0')\n","train loss 0.30600757524371147\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.4028254169970751\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  26\n","tensor(168, device='cuda:0')\n","train loss 0.30531518658002216\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.403149526566267\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  27\n","tensor(168, device='cuda:0')\n","train loss 0.30469027906656265\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.4035261236131191\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  28\n","tensor(168, device='cuda:0')\n","train loss 0.3041230160742998\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.4038676228374243\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  29\n","tensor(168, device='cuda:0')\n","train loss 0.30360700003802776\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.4041014239192009\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  30\n","tensor(168, device='cuda:0')\n","train loss 0.3031354919075966\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.40421938337385654\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  31\n","tensor(168, device='cuda:0')\n","train loss 0.30270392665018636\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.4042782820761204\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  32\n","tensor(168, device='cuda:0')\n","train loss 0.3023094180971384\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.40435308404266834\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  33\n","tensor(168, device='cuda:0')\n","train loss 0.30194646927217644\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.40448558516800404\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  34\n","tensor(168, device='cuda:0')\n","train loss 0.3016087592889865\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.4046638272702694\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  35\n","tensor(168, device='cuda:0')\n","train loss 0.30129270038257044\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.40483768843114376\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  36\n","tensor(168, device='cuda:0')\n","train loss 0.30099711908648413\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.4049575161188841\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  37\n","tensor(168, device='cuda:0')\n","train loss 0.30072117503732443\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.4050081595778465\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  38\n","tensor(168, device='cuda:0')\n","train loss 0.30046297920246917\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.405008552595973\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  39\n","tensor(168, device='cuda:0')\n","train loss 0.3002199751014511\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.4049961529672146\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  40\n","tensor(168, device='cuda:0')\n","train loss 0.29998891334980726\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.40504330955445766\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  41\n","tensor(168, device='cuda:0')\n","train loss 0.2997668331178526\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.4052081014961004\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  42\n","tensor(168, device='cuda:0')\n","train loss 0.29955210800593096\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.4054129533469677\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  43\n","tensor(168, device='cuda:0')\n","train loss 0.29934642603620887\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.4055365640670061\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  44\n","tensor(168, device='cuda:0')\n","train loss 0.2991507281549275\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.40558229573071003\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  45\n","tensor(168, device='cuda:0')\n","train loss 0.2989656386586527\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.4056339953094721\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  46\n","tensor(168, device='cuda:0')\n","train loss 0.2987898543166618\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.40573001094162464\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  47\n","tensor(168, device='cuda:0')\n","train loss 0.29862017231062055\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.40583666414022446\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  48\n","tensor(168, device='cuda:0')\n","train loss 0.29845456468562287\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.4059047233313322\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  49\n","tensor(168, device='cuda:0')\n","train loss 0.29829164563367766\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.40592204220592976\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  50\n","tensor(168, device='cuda:0')\n","train loss 0.2981297566245\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.4059113562107086\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  51\n","tensor(168, device='cuda:0')\n","train loss 0.29796661250293255\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.40592928789556026\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  52\n","tensor(168, device='cuda:0')\n","train loss 0.2978021771026154\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.40604414977133274\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  53\n","tensor(168, device='cuda:0')\n","train loss 0.29763865548496443\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.40618509612977505\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  54\n","tensor(168, device='cuda:0')\n","train loss 0.2974791674253841\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.40623901039361954\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  55\n","tensor(168, device='cuda:0')\n","train loss 0.29732554281751317\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.4062983766198158\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  56\n","tensor(168, device='cuda:0')\n","train loss 0.29717795938874286\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.4064395669847727\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  57\n","tensor(168, device='cuda:0')\n","train loss 0.29703419577951234\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.40654122084379196\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  58\n","tensor(168, device='cuda:0')\n","train loss 0.2968943732169767\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.4065925497561693\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  59\n","tensor(168, device='cuda:0')\n","train loss 0.2967578743118793\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.4066993985325098\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  60\n","tensor(168, device='cuda:0')\n","train loss 0.29662312008440495\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.40680674463510513\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  61\n","tensor(168, device='cuda:0')\n","train loss 0.2964892778545618\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.40686815045773983\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  62\n","tensor(168, device='cuda:0')\n","train loss 0.2963559305450569\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.4069598186761141\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  63\n","tensor(168, device='cuda:0')\n","train loss 0.2962215489242226\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.40705500543117523\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  64\n","tensor(168, device='cuda:0')\n","train loss 0.29608543892391026\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.4071127437055111\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  65\n","tensor(168, device='cuda:0')\n","train loss 0.29594683592828613\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.4071918651461601\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  66\n","tensor(168, device='cuda:0')\n","train loss 0.29580508490713936\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.4072745256125927\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  67\n","tensor(168, device='cuda:0')\n","train loss 0.2956611404661089\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.4073454160243273\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  68\n","tensor(168, device='cuda:0')\n","train loss 0.29551616551664966\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.4074413850903511\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  69\n","tensor(168, device='cuda:0')\n","train loss 0.29537060802492004\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.40752166509628296\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  70\n","tensor(168, device='cuda:0')\n","train loss 0.29522409111571807\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.4076107107102871\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  71\n","tensor(168, device='cuda:0')\n","train loss 0.29507539335948724\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.407704358920455\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  72\n","tensor(168, device='cuda:0')\n","train loss 0.2949233727995306\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.4077827222645283\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  73\n","tensor(168, device='cuda:0')\n","train loss 0.2947646124133219\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.4078713823109865\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  74\n","tensor(168, device='cuda:0')\n","train loss 0.2945956353408595\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.4079424627125263\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  75\n","tensor(168, device='cuda:0')\n","train loss 0.29441641193504137\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.4080467503517866\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  76\n","tensor(168, device='cuda:0')\n","train loss 0.2942315919014315\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.4081213902682066\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  77\n","tensor(168, device='cuda:0')\n","train loss 0.2940400425189485\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.4082956351339817\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  78\n","tensor(168, device='cuda:0')\n","train loss 0.2938369468320161\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.40836672857403755\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  79\n","tensor(168, device='cuda:0')\n","train loss 0.2936131958073626\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.4085532892495394\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  80\n","tensor(168, device='cuda:0')\n","train loss 0.29336297888463986\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.40836459770798683\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  81\n","tensor(168, device='cuda:0')\n","train loss 0.29306280019227415\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.4088188651949167\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  82\n","tensor(168, device='cuda:0')\n","train loss 0.29272645971893024\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.4083133917301893\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  83\n","tensor(168, device='cuda:0')\n","train loss 0.29218992226136226\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.4093101378530264\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  84\n","tensor(168, device='cuda:0')\n","train loss 0.2914775559523453\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.4129720311611891\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  85\n","tensor(168, device='cuda:0')\n","train loss 0.29075283638667315\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.40649329125881195\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  86\n","tensor(167, device='cuda:0')\n","train loss 0.29019960295408964\n","train acc tensor(0.8308, device='cuda:0')\n","test loss 0.427835313603282\n","test acc tensor(0.8588, device='cuda:0')\n","\n","\n","epoch  87\n","tensor(169, device='cuda:0')\n","train loss 0.2943895250791684\n","train acc tensor(0.8408, device='cuda:0')\n","test loss 0.41141460463404655\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  88\n","tensor(168, device='cuda:0')\n","train loss 0.29997296328656375\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.41952227614820004\n","test acc tensor(0.8588, device='cuda:0')\n","\n","\n","epoch  89\n","tensor(168, device='cuda:0')\n","train loss 0.29294715464736026\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.4144372772425413\n","test acc tensor(0.8588, device='cuda:0')\n","\n","\n","epoch  90\n","tensor(168, device='cuda:0')\n","train loss 0.2909502802649513\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.4058686178177595\n","test acc tensor(0.8588, device='cuda:0')\n","\n","\n","epoch  91\n","tensor(168, device='cuda:0')\n","train loss 0.29215996351558715\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.4078897200524807\n","test acc tensor(0.8588, device='cuda:0')\n","\n","\n","epoch  92\n","tensor(168, device='cuda:0')\n","train loss 0.29023935622535646\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.41611342690885067\n","test acc tensor(0.8588, device='cuda:0')\n","\n","\n","epoch  93\n","tensor(168, device='cuda:0')\n","train loss 0.29007331980392337\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.4120616316795349\n","test acc tensor(0.8588, device='cuda:0')\n","\n","\n","epoch  94\n","tensor(168, device='cuda:0')\n","train loss 0.2892364680301398\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.4090434964746237\n","test acc tensor(0.8588, device='cuda:0')\n","\n","\n","epoch  95\n","tensor(168, device='cuda:0')\n","train loss 0.2878795847063884\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.4115860052406788\n","test acc tensor(0.8588, device='cuda:0')\n","\n","\n","epoch  96\n","tensor(168, device='cuda:0')\n","train loss 0.2870219817850739\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.4118211567401886\n","test acc tensor(0.8588, device='cuda:0')\n","\n","\n","epoch  97\n","tensor(168, device='cuda:0')\n","train loss 0.2890220967431863\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.42286308109760284\n","test acc tensor(0.8588, device='cuda:0')\n","\n","\n","epoch  98\n","tensor(168, device='cuda:0')\n","train loss 0.2935135206207633\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.41290866769850254\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  99\n","tensor(168, device='cuda:0')\n","train loss 0.2930544315216442\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.40610065311193466\n","test acc tensor(0.8651, device='cuda:0')\n"]}]}]}