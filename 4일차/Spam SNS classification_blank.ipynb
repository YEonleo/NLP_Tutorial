{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Spam SNS classification_blank.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"4Vy_G2wCjua9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657516303218,"user_tz":-540,"elapsed":9932,"user":{"displayName":"류상연/AI·소프트웨어학부(소프트웨어전공)","userId":"00705256891800395029"}},"outputId":"aec70da6-6654-41ed-b1df-0140a5efab09"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"id":"X2Eq1SjUWGER","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657516315904,"user_tz":-540,"elapsed":8646,"user":{"displayName":"류상연/AI·소프트웨어학부(소프트웨어전공)","userId":"00705256891800395029"}},"outputId":"9449ff03-ed1d-493f-e929-40e8bf801b15"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n","\u001b[K     |████████████████████████████████| 4.4 MB 4.9 MB/s \n","\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 56.3 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n","\u001b[K     |████████████████████████████████| 101 kB 15.0 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 80.7 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.8.1 pyyaml-6.0 tokenizers-0.12.1 transformers-4.20.1\n"]}]},{"cell_type":"markdown","source":["# Spam SNS classification"],"metadata":{"id":"RjVOZjEU2pan"}},{"cell_type":"markdown","source":["각 문장이 spam 메일인지 아닌지를 판별하는 문장 분류 태스크  \n","dataset: https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset  \n","4000개까지 train으로 사용  \n","\n","Dataset 구축  \n","dataset에서는 __ getitem __을 통해 한개의 데이터를 모델의 입력형태에 맞추어 반환한다.  \n","이번 코드에서는 자연어 문장을 모델에 입력하기 위해 tokenization과 vocab dictionary에 따른 index로의 변환을 진행한다.  \n","또한 label의 ham/spam에 따라 0/1을 label로 변환한다.  \n","\n","주요 class, method:  \n","torchtext.data.utils.get_tokenizer: torchtext에서 제공하는 tokenizer 문법에 따른 tokenization을 수행하는 class 반환  \n","torchtext.vocab.build_vocab_from_iterator:  내 학습데이터에 대한 모든 단어를 입력하면 각 단어에 한개씩 index를 부여한 vocab dictionary 반환  \n","**huggingface.tokenizer:** 사전에 학습된 tokenization을 불러오는 huggingface class  \n","https://huggingface.co/docs/transformers/main_classes/tokenizer\n"],"metadata":{"id":"Hu1IWE_L2xM3"}},{"cell_type":"code","source":["import pandas as pd\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","\n","from transformers import AutoTokenizer,BertTokenizer\n","from torchtext.data.utils import get_tokenizer\n","from torchtext.vocab import build_vocab_from_iterator\n","\n","#dataset load\n","data_path = \"/content/drive/MyDrive/여름 NLP/4일차_배포/dataset/spam.csv\"\n","data_df = pd.read_csv(data_path, encoding = \"ISO-8859-1\")\n","print(data_df)\n","data_df = data_df.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis=1)\n","print(data_df.columns)\n","\n","train_df = data_df.loc[:4000,:].reset_index()\n","test_df = data_df.loc[4000:,:].reset_index()\n","\n","ex_text = train_df.loc[0,\"v2\"]\n","print(tokenizer(ex_text))\n","\n","tokenizer=get_tokenizer(\"basic_english\")\n","\n","vocab = build_vocab_from_iterator(list(map(tokenizer,data_df.loc[:,\"v2\"])))\n","print(vocab(tokenizer(data_df.loc[0,\"v2\"])))\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n","print(tokenizer(data_df.loc[0,\"v2\"], max_length=100, padding='max_length', truncation=True))\n","\n","class myDataset(Dataset):\n","  def __init__(self,df,tokenizer)->None:\n","    super().__init__()\n","    self.df = df\n","    self.tokenizer = tokenizer\n","\n","  def __len__(self):\n","    return len(self.df)\n","\n","  def __getitem__(self,index):\n","    data = self.df.loc[index, \"v2\"]\n","    #print(data)\n","    data = self.tokenizer(data, max_length=100,padding='max_length', truncation=True)[\"input_ids\"]\n","    data.reverse()\n","\n","    if self.df.loc[index,\"v1\"]==\"spam\":\n","      label = 1\n","    elif self.df.loc[index,\"v1\"]==\"ham\":\n","      label = 0\n","\n","    return torch.tensor(data, dtype=torch.int64) , label\n","\n","train_dataset = myDataset(train_df, tokenizer)\n","test_dataset = myDataset(test_df, tokenizer)\n","\n","for i in train_dataset:\n","  print(i)\n","  break\n","\n","batch_size = 3\n","train_dataloader = DataLoader(train_dataset,batch_size=batch_size)\n","test_dataloader = DataLoader(test_dataset,batch_size=batch_size)\n","\n","for i in train_dataloader:\n","  print(i)\n","  break\n","\n","  "],"metadata":{"id":"8kWdTTr8XWal","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657519613016,"user_tz":-540,"elapsed":1409,"user":{"displayName":"류상연/AI·소프트웨어학부(소프트웨어전공)","userId":"00705256891800395029"}},"outputId":"cb5dc884-30b0-42f8-a16b-b93e3f5dc8b5"},"execution_count":66,"outputs":[{"output_type":"stream","name":"stdout","text":["        v1                                                 v2 Unnamed: 2  \\\n","0      ham  Go until jurong point, crazy.. Available only ...        NaN   \n","1      ham                      Ok lar... Joking wif u oni...        NaN   \n","2     spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n","3      ham  U dun say so early hor... U c already then say...        NaN   \n","4      ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n","...    ...                                                ...        ...   \n","5567  spam  This is the 2nd time we have tried 2 contact u...        NaN   \n","5568   ham              Will Ì_ b going to esplanade fr home?        NaN   \n","5569   ham  Pity, * was in mood for that. So...any other s...        NaN   \n","5570   ham  The guy did some bitching but I acted like i'd...        NaN   \n","5571   ham                         Rofl. Its true to its name        NaN   \n","\n","     Unnamed: 3 Unnamed: 4  \n","0           NaN        NaN  \n","1           NaN        NaN  \n","2           NaN        NaN  \n","3           NaN        NaN  \n","4           NaN        NaN  \n","...         ...        ...  \n","5567        NaN        NaN  \n","5568        NaN        NaN  \n","5569        NaN        NaN  \n","5570        NaN        NaN  \n","5571        NaN        NaN  \n","\n","[5572 rows x 5 columns]\n","Index(['v1', 'v2'], dtype='object')\n","{'input_ids': [101, 3414, 1235, 179, 22497, 1403, 1553, 117, 4523, 119, 119, 11651, 8009, 2165, 1178, 1107, 15430, 1548, 183, 1632, 1362, 2495, 174, 171, 9435, 2105, 119, 119, 119, 140, 2042, 1175, 1400, 1821, 4474, 20049, 1204, 119, 119, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n","[56, 471, 6957, 862, 4, 782, 0, 0, 651, 75, 12, 1321, 100, 131, 367, 1375, 169, 3339, 0, 0, 0, 1328, 67, 66, 5229, 143, 0, 0, 0]\n","{'input_ids': [101, 3414, 1235, 179, 22497, 1403, 1553, 117, 4523, 119, 119, 11651, 8009, 2165, 1178, 1107, 15430, 1548, 183, 1632, 1362, 2495, 174, 171, 9435, 2105, 119, 119, 119, 140, 2042, 1175, 1400, 1821, 4474, 20049, 1204, 119, 119, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n","(tensor([    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,   102,\n","          119,   119,   119,  1204, 20049,  4474,  1821,  1400,  1175,  2042,\n","          140,   119,   119,   119,  2105,  9435,   171,   174,  2495,  1362,\n","         1632,   183,  1548, 15430,  1107,  1178,  2165,  8009, 11651,   119,\n","          119,  4523,   117,  1553,  1403, 22497,   179,  1235,  3414,   101]), 0)\n","[tensor([[    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,   102,\n","           119,   119,   119,  1204, 20049,  4474,  1821,  1400,  1175,  2042,\n","           140,   119,   119,   119,  2105,  9435,   171,   174,  2495,  1362,\n","          1632,   183,  1548, 15430,  1107,  1178,  2165,  8009, 11651,   119,\n","           119,  4523,   117,  1553,  1403, 22497,   179,  1235,  3414,   101],\n","        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,   102,   119,   119,   119,  1182,  1113,   190,  8914,\n","           192,  4419,  8125,   119,   119,   119,  1197,  2495, 23330,   101],\n","        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,   102,   188,   112, 15292,  5909, 26253, 20150, 24606,\n","         21336,  4775,  6058,   188,   112,   140,   111,   157,   114,  2603,\n","          1204,  1775,   189,  1181,  1204,   188,   113,  2304,  3990,  3531,\n","          1106,  1475, 11964,  5966,  1106,  6820, 18430,   119,  1478,  1318,\n","          6880,  1116, 21270,   189,  1509,  1635,  6820,  1782,  1106,  1643,\n","          3254,  1193,  1377,   192,   170,   123,  1107,  3990,  4299,   101]]), tensor([0, 0, 1])]\n"]}]},{"cell_type":"markdown","source":["model 선언  \n","문장을 RNN/LSTM을 통해 classification하는 model 선언\n","1. token의 index를 입력으로 받고 word embedding을 결과로 반환하는 nn.Embedding 사용  \n","2. LSTM을 통해 모든 token을 입력  \n","3. many-to-one구조를 가지기 때문에 LSTM의 결과중 마지막 cell에 대한 결과만을 사용하여 nn.linear를통해 classification\n","\n","입력으로는 문장을 tokenziation과 indexing한게 입력으로 들어오기 때문에(batch, sequance length)형태의 입력  \n","이후 nn.Embedding을 거치면서 각 단어의 벡터가 생성되기 때문에(batch, sequance length, hidden size)의 형태 사용\n","\n","주요 obejct:  \n","**nn.Embedding:**https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html  \n","**nn.LSTM:**https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html  \n","**nn.RNN:**https://pytorch.org/docs/stable/generated/torch.nn.RNN.html"],"metadata":{"id":"xfKV9_575XYT"}},{"cell_type":"code","source":["from torch import nn\n","\n","class MyModel(nn.Module):\n","  def __init__(self)->None:\n","    super().__init__()\n","    self.emb = nn.Embedding(50000, 128)\n","    self.rnn = nn.RNN(128,128)\n","    self.ln = nn.Linear(128,2)\n","\n","  def forward(self,x):\n","    #print(x.shape)\n","    x=self.emb(x)\n","    #print(x.shape)\n","    x, _=self.rnn(x)\n","    #print(x.shape)\n","    x = x[:,-1,:]\n","    x = self.ln(x)\n","\n","    return x\n","\n","\n","model = MyModel()\n","for i in train_dataloader:\n","  print(model(i[0]))\n","  break"],"metadata":{"id":"ss1jJefWEw6X","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657519748482,"user_tz":-540,"elapsed":274,"user":{"displayName":"류상연/AI·소프트웨어학부(소프트웨어전공)","userId":"00705256891800395029"}},"outputId":"e961e6c0-246a-4e71-d5bd-c5ebe6addb1b"},"execution_count":69,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.3315, 0.2253],\n","        [0.3085, 0.2473],\n","        [0.2563, 0.3129]], grad_fn=<AddmmBackward0>)\n"]}]},{"cell_type":"markdown","source":["model train"],"metadata":{"id":"sg8IRfDM7Z_3"}},{"cell_type":"code","source":["from torch.optim import Adam\n","from torch.nn import CrossEntropyLoss\n","\n","model = MyModel()\n","model.cuda()\n","\n","#학습을 위한 optimizer와 loss function 설정\n","optimizer = Adam(model.parameters(), lr=0.001)\n","lf = CrossEntropyLoss().cuda()\n","\n","\n","#100번의 에폭을 실행\n","for e in range(100):\n","  print(\"\\n\\nepoch \", e)\n","  epoch_loss = 0\n","  train_correct = 0 \n","  \n","  #선언한 모델 오브젝트를 학습가능한 상태로 변경\n","  model.train()\n","\n","  #모든 학습데이터에 대해서 학습\n","  for i in train_dataloader:\n","    #매 배치에 대한 gradient계산 이전에 optimizer에 저장된 이전 batch에 gradient를 삭제(초기화)\n","    optimizer.zero_grad()\n","    data = i[0]\n","    data = data.cuda()\n","    target = i[1]\n","    target = target.cuda()\n","\n","    #결과 도출 및 정답수 연산\n","    output = model(data)\n","    pred_label = torch.argmax(output, dim=-1)\n","    train_correct += sum(pred_label == target.reshape(-1))\n","\n","    target = target.reshape(-1)\n","    #loss연산\n","    loss = lf(output, target)\n","    #print(loss)\n","\n","    #loss backpropagation\n","    loss.backward()\n","\n","    #gradient update\n","    optimizer.step()\n","\n","    epoch_loss += loss.item()\n","  \n","  print(train_correct)\n","  print(\"train loss\", epoch_loss/len(train_dataloader))\n","  print(\"train acc\", train_correct/len(train_dataset))\n","\n","  #model이 학습되지 않는 상태로 변경\n","  model.eval()\n","  test_loss = 0\n","  test_correct = 0 \n","\n","  #gradient를 계산하지 않도록 하여 cost낭비 방지\n","  with torch.no_grad():\n","    #모든 test dataset에 대해서 결과연산\n","    for i in test_dataloader:\n","      data = i[0]\n","      target = i[1]\n","      data = data.cuda()\n","      target = target.cuda()\n","\n","      output = model(data)\n","\n","      loss = lf(output, target.reshape(-1))\n","      pred_label = torch.argmax(output, dim=-1)\n","      test_correct += sum(pred_label == target.reshape(-1))\n","      test_loss += loss.item()\n","\n","  print(\"test loss\", test_loss/len(test_dataloader))\n","  print(\"test acc\", test_correct/len(test_dataset))\n","    \n"],"metadata":{"id":"zmVPBc1AN4j_","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4b7e6dbd-932a-4a09-c596-547d584de5ca"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","epoch  0\n","tensor(3462, device='cuda:0')\n","train loss 0.4057783709372627\n","train acc tensor(0.8653, device='cuda:0')\n","test loss 0.4094588239001864\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  1\n","tensor(3466, device='cuda:0')\n","train loss 0.4055002236143983\n","train acc tensor(0.8663, device='cuda:0')\n","test loss 0.41587520828683866\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  2\n","tensor(3466, device='cuda:0')\n","train loss 0.4064186796255555\n","train acc tensor(0.8663, device='cuda:0')\n","test loss 0.41528013496453525\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  3\n","tensor(3466, device='cuda:0')\n","train loss 0.40671150450004095\n","train acc tensor(0.8663, device='cuda:0')\n","test loss 0.4092928969678078\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  4\n","tensor(3466, device='cuda:0')\n","train loss 0.4063070533347541\n","train acc tensor(0.8663, device='cuda:0')\n","test loss 0.4104859114603232\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  5\n","tensor(3466, device='cuda:0')\n","train loss 0.40655968591258085\n","train acc tensor(0.8663, device='cuda:0')\n","test loss 0.409063155073246\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  6\n","tensor(3466, device='cuda:0')\n","train loss 0.40666796587619825\n","train acc tensor(0.8663, device='cuda:0')\n","test loss 0.41272082492595413\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  7\n","tensor(3466, device='cuda:0')\n","train loss 0.40671483861810265\n","train acc tensor(0.8663, device='cuda:0')\n","test loss 0.410301057778242\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  8\n","tensor(3466, device='cuda:0')\n","train loss 0.4065579628233967\n","train acc tensor(0.8663, device='cuda:0')\n","test loss 0.411675637118689\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  9\n"]}]}]}