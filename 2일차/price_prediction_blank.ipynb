{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"price_prediction_blank.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"od83zh3WUhw4","executionInfo":{"status":"ok","timestamp":1657089583700,"user_tz":-540,"elapsed":2500,"user":{"displayName":"류상연/AI·소프트웨어학부(소프트웨어전공)","userId":"00705256891800395029"}},"outputId":"8017a004-828a-471f-90c4-25478925f428"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["#house price prediction in boston"],"metadata":{"id":"qCgEXYXjUPeZ"}},{"cell_type":"markdown","source":["Pytorch의 MLP를 이용한 집값예측(regression)구현  \n","  \n","주요 모듈 목록  \n","**torch.utils.data.Dataset:** 데이터셋을 getitem method에서 한개씩 텐서형태로 반환하도록 구성하는 class  \n","**torch.utils.data.DataLoader:** batch processing을 위해 dataset에서 반환되는 데이터를 n개의 batch_size만큼 묶어서 반환하도록 하는 class  \n","**torch.nn.Module:** 딥러닝 모델을 구현한 class, forward method를 통해 입력된 텐서 데이터를 딥러닝 연산하여 결과 반환  \n","**torch.nn.(loss):** loss 연산을 위한 class이며 backward method를 통해 gradient계산  \n","**torch.optim.(optimizer):** 계산된 gradient를 update해주어 feadient descent를 진행하는 class  "],"metadata":{"id":"--pvfS_VWMij"}},{"cell_type":"markdown","source":["Dataset class구현  \n","  \n","dataset(Housingdata.csv) : https://www.kaggle.com/datasets/altavish/boston-housing-dataset\n","\n","traget: MEDV(집값)  \n","input feature: MEDV를 제외한 모든 값  \n","전체 데이터 약 500개중 400개까지 train data로 사용, 나머지는 test data  \n","\n","각 data를 torch.utils.data.Dataset을 통해 하나씩 load할 수 있는 class구현  \n","이후 torch.utils.data.DataLoader를 통해 batch개씩 변환\n","\n","torch.utils.data.Dataset의 주요 method \n","__ init __: 클래스를 오브젝트로 생성할때 불러와지는 함수, 클래스에서 필요한 인스턴스(데이터셋, 데이터경로)등을 생성  \n","__ len __: 해당 클래스에서 다루는 dataset의 길이를 반환하는 함수  \n","__ getitem __(index): index에 해당하는 데이터 하나를 tensor형태로 반환하는 함수"],"metadata":{"id":"_pyaoU4IYAaw"}},{"cell_type":"code","source":["from torch.utils.data import Dataset, DataLoader\n","import pandas as pd\n","import torch\n","\n","train_data_path = \"/content/drive/MyDrive/여름 NLP/2일차_배포/dataset/HousingData.csv\"\n","\n","\n","\n","\n","class myDataset(Dataset):\n","  def __init__(self,dataset_df)->None:\n","    super().__init__()\n","    self.dataset_df=dataset_df\n","    self.inputData_df = dataset_df.drop(\"MEDV\", axis=1)\n","    print(self.inputData_df)\n","    self.target_df = dataset_df.loc[:,[\"MEDV\"]]\n","    print(self.target_df)\n","    \n","  def __len__(self):\n","    return len(self.target_df)\n","\n","  def __getitem__(self,index):\n","    data_tensor = torch.Tensor(self.inputData_df.loc[index,:])\n","    target_tensor = torch.Tensor(self.target_df.loc[index,:])\n","    \n","    return data_tensor, target_tensor\n","\n","batch_size = 4\n","\n","data_df = pd.read_csv(train_data_path).dropna()\n","train_df = data_df.loc[:400,:].reset_index()\n","dev_df = data_df.loc[400:,:].reset_index()\n","\n","train_dataset = myDataset(train_df)\n","dev_dataset = myDataset(dev_df)\n","\n","train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n","dev_dataloader = DataLoader(dev_dataset, batch_size=batch_size)\n","\n","\n","\n","for i in train_dataset:\n","  print(i)\n","  break\n","\n","    "],"metadata":{"id":"GgwV6nIIUapc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657090349898,"user_tz":-540,"elapsed":336,"user":{"displayName":"류상연/AI·소프트웨어학부(소프트웨어전공)","userId":"00705256891800395029"}},"outputId":"9a53828e-ded4-4f2e-cd39-672f1c3394d7"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["     index      CRIM    ZN  INDUS  CHAS    NOX     RM    AGE     DIS  RAD  \\\n","0        0   0.00632  18.0   2.31   0.0  0.538  6.575   65.2  4.0900    1   \n","1        1   0.02731   0.0   7.07   0.0  0.469  6.421   78.9  4.9671    2   \n","2        2   0.02729   0.0   7.07   0.0  0.469  7.185   61.1  4.9671    2   \n","3        3   0.03237   0.0   2.18   0.0  0.458  6.998   45.8  6.0622    3   \n","4        5   0.02985   0.0   2.18   0.0  0.458  6.430   58.7  6.0622    3   \n","..     ...       ...   ...    ...   ...    ...    ...    ...     ...  ...   \n","308    396   5.87205   0.0  18.10   0.0  0.693  6.405   96.0  1.6768   24   \n","309    397   7.67202   0.0  18.10   0.0  0.693  5.747   98.9  1.6334   24   \n","310    398  38.35180   0.0  18.10   0.0  0.693  5.453  100.0  1.4896   24   \n","311    399   9.91655   0.0  18.10   0.0  0.693  5.852   77.8  1.5004   24   \n","312    400  25.04610   0.0  18.10   0.0  0.693  5.987  100.0  1.5888   24   \n","\n","     TAX  PTRATIO       B  LSTAT  \n","0    296     15.3  396.90   4.98  \n","1    242     17.8  396.90   9.14  \n","2    242     17.8  392.83   4.03  \n","3    222     18.7  394.63   2.94  \n","4    222     18.7  394.12   5.21  \n","..   ...      ...     ...    ...  \n","308  666     20.2  396.90  19.37  \n","309  666     20.2  393.10  19.92  \n","310  666     20.2  396.90  30.59  \n","311  666     20.2  338.16  29.97  \n","312  666     20.2  396.90  26.77  \n","\n","[313 rows x 14 columns]\n","     MEDV\n","0    24.0\n","1    21.6\n","2    34.7\n","3    33.4\n","4    28.7\n","..    ...\n","308  12.5\n","309   8.5\n","310   5.0\n","311   6.3\n","312   5.6\n","\n","[313 rows x 1 columns]\n","    index      CRIM   ZN  INDUS  CHAS    NOX     RM    AGE     DIS  RAD  TAX  \\\n","0     400  25.04610  0.0  18.10   0.0  0.693  5.987  100.0  1.5888   24  666   \n","1     402   9.59571  0.0  18.10   0.0  0.693  6.404  100.0  1.6390   24  666   \n","2     403  24.80170  0.0  18.10   0.0  0.693  5.349   96.0  1.7028   24  666   \n","3     404  41.52920  0.0  18.10   0.0  0.693  5.531   85.4  1.6074   24  666   \n","4     405  67.92080  0.0  18.10   0.0  0.693  5.683  100.0  1.4254   24  666   \n","..    ...       ...  ...    ...   ...    ...    ...    ...     ...  ...  ...   \n","77    499   0.17783  0.0   9.69   0.0  0.585  5.569   73.5  2.3999    6  391   \n","78    500   0.22438  0.0   9.69   0.0  0.585  6.027   79.7  2.4982    6  391   \n","79    502   0.04527  0.0  11.93   0.0  0.573  6.120   76.7  2.2875    1  273   \n","80    503   0.06076  0.0  11.93   0.0  0.573  6.976   91.0  2.1675    1  273   \n","81    504   0.10959  0.0  11.93   0.0  0.573  6.794   89.3  2.3889    1  273   \n","\n","    PTRATIO       B  LSTAT  \n","0      20.2  396.90  26.77  \n","1      20.2  376.11  20.31  \n","2      20.2  396.90  19.77  \n","3      20.2  329.46  27.38  \n","4      20.2  384.97  22.98  \n","..      ...     ...    ...  \n","77     19.2  395.77  15.10  \n","78     19.2  396.90  14.33  \n","79     21.0  396.90   9.08  \n","80     21.0  396.90   5.64  \n","81     21.0  393.45   6.48  \n","\n","[82 rows x 14 columns]\n","    MEDV\n","0    5.6\n","1   12.1\n","2    8.3\n","3    8.5\n","4    5.0\n","..   ...\n","77  17.5\n","78  16.8\n","79  20.6\n","80  23.9\n","81  22.0\n","\n","[82 rows x 1 columns]\n","(tensor([0.0000e+00, 6.3200e-03, 1.8000e+01, 2.3100e+00, 0.0000e+00, 5.3800e-01,\n","        6.5750e+00, 6.5200e+01, 4.0900e+00, 1.0000e+00, 2.9600e+02, 1.5300e+01,\n","        3.9690e+02, 4.9800e+00]), tensor([24.]))\n"]}]},{"cell_type":"markdown","source":["Deep learning 모델 구현  \n","torch.nn.Module을 이용하여 모델 구현  \n","1st hidden layer의 feature는 100개  \n","2nd hidden layer의 feature는 10개  \n","인 모델을 구현한다.  \n","\n","torch.nn.Linear: perceptron의 weighted sum과 같이 linaer regression연산을 하는 calss  \n","torch.nn.ReLU: ReLU activation function을 수행하는 class  \n","\n","torch.nn.Module의 주요 method  \n","__ init __: 클래스를 오브젝트로 생성할때 불러와지는 함수, 클래스에서 필요한 인스턴스(사용할 deep learning layer, activation function, 등)등을 생성  \n","__ forward __(data): 입력받은 data를 딥러닝 모델을 통해 결과를 예측하여 반환하는 class  \n","\n","**각 딥러닝 레이어 연산 중 차원수를 확인하고 잘 맞춰줄 것**  \n","참고 document(해당 사이트의 shape를 확인하고 tensor형태 결정)  \n","nn.Linear: https://pytorch.org/docs/stable/generated/torch.nn.Linear.html  \n","nn.ReLU: https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html"],"metadata":{"id":"n7j-zH_mfTbY"}},{"cell_type":"code","execution_count":18,"metadata":{"id":"pxJqIF7cW-lT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657090354139,"user_tz":-540,"elapsed":329,"user":{"displayName":"류상연/AI·소프트웨어학부(소프트웨어전공)","userId":"00705256891800395029"}},"outputId":"47fcbaac-6cf9-4290-a384-3826112caa8d"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[3.1446],\n","        [1.4531],\n","        [1.6547],\n","        [1.4930]], grad_fn=<ReluBackward0>)\n"]}],"source":["from torch import nn\n","\n","class myModel(nn.Module):\n","  def __init__(self)->None:\n","    super().__init__()\n","\n","    self.ln1 = nn.Linear(14,100,bias=True)\n","    self.act1 = nn.ReLU()\n","\n","    self.ln2 = nn.Linear(100,10,bias=True)\n","    self.ln3 = nn.Linear(10,1,bias=True)\n","\n","  def forward(self,x):\n","    #2- , 14\n","    x = self.ln1(x)\n","\n","    x = self.act1(x)\n","\n","    x = self.ln2(x)\n","    x = self.act1(x)\n","\n","    x = self.ln3(x)\n","    x = self.act1(x)\n","\n","    return x\n","\n","model = myModel()\n","\n","for i in train_dataloader:\n","  data = i[0]\n","  target = i[1]\n","  output = model(data)\n","  print(output)\n","  break\n","\n","\n","\n"]},{"cell_type":"markdown","source":["작성한 dataset과 model을 이용하여 딥러닝 프로세스 구현  \n","\n","pytorch 딥러닝 프로세스\n","1. dataset, model선언\n","2. dataset과 model을 통한 결과 예측\n","3. 예측된 결과를 통해 **loss**연산 및 **loss.backward**\n","4. **optimizer.step()**를 사용하여 graident update\n","\n","주요 오브젝트  \n","torch.nn.MSELoss: 예측값과 정답을 통해 MSE값을 반환하는 class  \n","torch.optim.Adam: Adam optimizer를 통해 gradient update를 수행하는 class\n","\n","각 오브젝트의 입력과 선언은 다음 doc 참조:  \n","MSELoss: https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html  \n","Adam: https://pytorch.org/docs/stable/generated/torch.optim.Adam.html"],"metadata":{"id":"MvVKNpwAoymc"}},{"cell_type":"code","source":["from torch.optim import Adam\n","from torch.nn import MSELoss\n","\n","optimizer = Adam(model.parameters(),lr=0.001)\n","lf = MSELoss()\n","\n","\n","for e in range(100):\n","  print(\"epoch\",e)\n","  epoch_loss = 0\n","\n","  model.train()\n","  for i in train_dataloader:\n","    optimizer.zero_grad()\n","    data = i[0]\n","    target = i[1]\n","    pred = model(data)\n","\n","    loss = lf(pred, target)\n","    loss.backward()\n","    optimizer.step()\n","\n","    epoch_loss += loss.item();\n","    \n","  print(\"train loss\", epoch_loss/len(train_dataloader))\n","\n","  model.eval()\n","  test_loss = 0\n","\n","  with torch.no_grad():\n","\n","    for i in dev_dataloader:\n","      data = i[0]\n","      target = i[1]\n","\n","      output = model(data)\n","\n","      loss = lf(output, target)\n","      test_loss += loss.item()\n","\n","  print(\"test loss\",test_loss/len(dev_dataloader))\n","\n","  \n","\n","\n"],"metadata":{"id":"enENUuVFo4VN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657090384946,"user_tz":-540,"elapsed":26067,"user":{"displayName":"류상연/AI·소프트웨어학부(소프트웨어전공)","userId":"00705256891800395029"}},"outputId":"2eacd67e-1b7f-428e-a854-7cb5774d4225"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["epoch 0\n","train loss 100.9456174494345\n","test loss 31.16543820358458\n","epoch 1\n","train loss 95.56837154038345\n","test loss 47.18920201346988\n","epoch 2\n","train loss 69.70888612730593\n","test loss 71.13887521198818\n","epoch 3\n","train loss 75.33320985262907\n","test loss 72.45257132393974\n","epoch 4\n","train loss 88.81744755597055\n","test loss 72.97620786939349\n","epoch 5\n","train loss 91.50338554080528\n","test loss 72.33147921448662\n","epoch 6\n","train loss 91.1757952518101\n","test loss 68.78010970070248\n","epoch 7\n","train loss 94.8584064109416\n","test loss 77.09649276733398\n","epoch 8\n","train loss 93.74874730955196\n","test loss 76.67063856124878\n","epoch 9\n","train loss 91.49400589435915\n","test loss 80.51127174922398\n","epoch 10\n","train loss 88.24666265634042\n","test loss 79.39382639385406\n","epoch 11\n","train loss 80.10569382619254\n","test loss 86.72760622841972\n","epoch 12\n","train loss 72.90860083887848\n","test loss 79.98947198050362\n","epoch 13\n","train loss 65.82324831697005\n","test loss 84.07980519249325\n","epoch 14\n","train loss 58.78160749055162\n","test loss 78.57248160952614\n","epoch 15\n","train loss 53.55382361668575\n","test loss 74.85223306928363\n","epoch 16\n","train loss 49.420035909248305\n","test loss 68.62653966177078\n","epoch 17\n","train loss 46.04918158167525\n","test loss 71.54634116944813\n","epoch 18\n","train loss 45.08631278329258\n","test loss 64.26883974529449\n","epoch 19\n","train loss 44.71775613536563\n","test loss 68.52161668595814\n","epoch 20\n","train loss 45.096912047719655\n","test loss 66.2748527980986\n","epoch 21\n","train loss 44.69879445848586\n","test loss 66.25377684547787\n","epoch 22\n","train loss 43.731261515919165\n","test loss 66.17736375899543\n","epoch 23\n","train loss 43.66745430457441\n","test loss 70.54209037054153\n","epoch 24\n","train loss 43.880770840222326\n","test loss 67.83891285033454\n","epoch 25\n","train loss 43.09856517556347\n","test loss 69.09479881468273\n","epoch 26\n","train loss 43.126557492002654\n","test loss 66.56906075704666\n","epoch 27\n","train loss 42.53826988497867\n","test loss 71.20833226612636\n","epoch 28\n","train loss 43.49577511564086\n","test loss 64.92128649212066\n","epoch 29\n","train loss 42.07685037956962\n","test loss 69.30959840047927\n","epoch 30\n","train loss 42.236285250398176\n","test loss 61.374469671930584\n","epoch 31\n","train loss 41.19363683235796\n","test loss 61.437481079782756\n","epoch 32\n","train loss 40.34066004994549\n","test loss 60.10269363153549\n","epoch 33\n","train loss 39.72309323051308\n","test loss 56.43231930051531\n","epoch 34\n","train loss 38.88383839100222\n","test loss 55.31649273917789\n","epoch 35\n","train loss 37.86868753916101\n","test loss 53.443394178435916\n","epoch 36\n","train loss 36.979466670676125\n","test loss 50.85255176680429\n","epoch 37\n","train loss 35.63643318641035\n","test loss 53.384248551868254\n","epoch 38\n","train loss 34.45041635104373\n","test loss 51.77935343129294\n","epoch 39\n","train loss 33.03311902586418\n","test loss 53.525584402538485\n","epoch 40\n","train loss 31.77525060124035\n","test loss 52.505423233622594\n","epoch 41\n","train loss 30.41524703366847\n","test loss 55.1376454716637\n","epoch 42\n","train loss 29.282971765421614\n","test loss 55.93324433054243\n","epoch 43\n","train loss 27.768988074003897\n","test loss 55.219852969759984\n","epoch 44\n","train loss 27.042002706588068\n","test loss 55.33802419617062\n","epoch 45\n","train loss 26.587422901316533\n","test loss 51.5480637550354\n","epoch 46\n","train loss 26.43186801596533\n","test loss 51.16103810355777\n","epoch 47\n","train loss 26.203175015087368\n","test loss 49.309960762659706\n","epoch 48\n","train loss 26.07916403043119\n","test loss 48.83539453006926\n","epoch 49\n","train loss 25.787252561201022\n","test loss 50.40251713707333\n","epoch 50\n","train loss 25.41265398980696\n","test loss 51.23192662284488\n","epoch 51\n","train loss 25.360052366611324\n","test loss 47.74142868178232\n","epoch 52\n","train loss 25.208850423746473\n","test loss 52.065541880471365\n","epoch 53\n","train loss 24.757941261122497\n","test loss 54.132212843213765\n","epoch 54\n","train loss 24.534994698777982\n","test loss 54.134102526165194\n","epoch 55\n","train loss 24.215639045344123\n","test loss 54.013092063722155\n","epoch 56\n","train loss 23.986610238310657\n","test loss 56.3535358338129\n","epoch 57\n","train loss 23.738881979939304\n","test loss 56.918149630228676\n","epoch 58\n","train loss 24.212141446957858\n","test loss 51.70210523832412\n","epoch 59\n","train loss 23.593805116943166\n","test loss 47.6609456879752\n","epoch 60\n","train loss 23.654519145247303\n","test loss 48.18913706143697\n","epoch 61\n","train loss 23.499110338054127\n","test loss 46.249351308459325\n","epoch 62\n","train loss 23.215779668168178\n","test loss 45.7115672826767\n","epoch 63\n","train loss 23.161920253989063\n","test loss 41.93490356490726\n","epoch 64\n","train loss 22.86757636372047\n","test loss 41.43140690667288\n","epoch 65\n","train loss 22.8628165439903\n","test loss 40.72128759111677\n","epoch 66\n","train loss 22.838977223521546\n","test loss 39.70403908547901\n","epoch 67\n","train loss 22.599828339547297\n","test loss 39.471191224597746\n","epoch 68\n","train loss 22.56825757083259\n","test loss 38.05618968464079\n","epoch 69\n","train loss 22.574829696382903\n","test loss 36.97812013399033\n","epoch 70\n","train loss 22.29067286594382\n","test loss 36.70213131109873\n","epoch 71\n","train loss 22.30346809120118\n","test loss 35.580189790044514\n","epoch 72\n","train loss 22.2857704753457\n","test loss 37.820443459919524\n","epoch 73\n","train loss 22.31153533664308\n","test loss 35.7868055389041\n","epoch 74\n","train loss 22.028182645528755\n","test loss 35.87473477636065\n","epoch 75\n","train loss 22.122986598482616\n","test loss 35.34144238063267\n","epoch 76\n","train loss 21.916405556699896\n","test loss 35.11439225787208\n","epoch 77\n","train loss 21.90020764930339\n","test loss 32.51118978432247\n","epoch 78\n","train loss 21.761114227620862\n","test loss 33.45780408950079\n","epoch 79\n","train loss 21.94698378934136\n","test loss 31.26141091755458\n","epoch 80\n","train loss 21.78099938812135\n","test loss 32.4391804763249\n","epoch 81\n","train loss 21.537176625638068\n","test loss 32.29778368132455\n","epoch 82\n","train loss 21.672492667844026\n","test loss 32.39869142714001\n","epoch 83\n","train loss 21.46492532272882\n","test loss 31.496766930534726\n","epoch 84\n","train loss 21.4319426191004\n","test loss 34.70542671566918\n","epoch 85\n","train loss 21.38464397720144\n","test loss 41.875405970073885\n","epoch 86\n","train loss 21.634977677200414\n","test loss 28.132078531242552\n","epoch 87\n","train loss 22.04964708753779\n","test loss 33.34533200945173\n","epoch 88\n","train loss 21.594326281472096\n","test loss 28.475196736199514\n","epoch 89\n","train loss 21.49552938870237\n","test loss 30.14061960152217\n","epoch 90\n","train loss 21.973849719461008\n","test loss 32.79309928417206\n","epoch 91\n","train loss 21.595515328117564\n","test loss 37.79764976955595\n","epoch 92\n","train loss 21.1605207603189\n","test loss 39.84896709805443\n","epoch 93\n","train loss 20.604646774786936\n","test loss 44.270038354964484\n","epoch 94\n","train loss 20.162476927796497\n","test loss 59.15165998822167\n","epoch 95\n","train loss 20.421563905911356\n","test loss 34.409946407590596\n","epoch 96\n","train loss 20.866103249543077\n","test loss 32.46675759270077\n","epoch 97\n","train loss 20.546678042581565\n","test loss 40.31969560895647\n","epoch 98\n","train loss 20.795148652754254\n","test loss 47.33113752092634\n","epoch 99\n","train loss 20.770327747622623\n","test loss 56.35448746454148\n"]}]}]}