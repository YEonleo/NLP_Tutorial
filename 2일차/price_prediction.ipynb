{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"price_prediction.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO+mWazYLBgbbIR5SHg8DGs"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"od83zh3WUhw4","executionInfo":{"status":"ok","timestamp":1657076723471,"user_tz":-540,"elapsed":19574,"user":{"displayName":"김균엽","userId":"17069744786161993916"}},"outputId":"268729c7-1f25-4afa-c8c7-40b7f0c74d72"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["#house price prediction in boston"],"metadata":{"id":"qCgEXYXjUPeZ"}},{"cell_type":"markdown","source":["Pytorch의 MLP를 이용한 집값예측(regression)구현  \n","  \n","주요 모듈 목록  \n","**torch.utils.data.Dataset:** 데이터셋을 getitem method에서 한개씩 텐서형태로 반환하도록 구성하는 class  \n","**torch.utils.data.DataLoader:** batch processing을 위해 dataset에서 반환되는 데이터를 n개의 batch_size만큼 묶어서 반환하도록 하는 class  \n","**torch.nn.Module:** 딥러닝 모델을 구현한 class, forward method를 통해 입력된 텐서 데이터를 딥러닝 연산하여 결과 반환  \n","**torch.nn.(loss):** loss 연산을 위한 class이며 backward method를 통해 gradient계산  \n","**torch.optim.(optimizer):** 계산된 gradient를 update해주어 feadient descent를 진행하는 class  "],"metadata":{"id":"--pvfS_VWMij"}},{"cell_type":"markdown","source":["Dataset class구현  \n","  \n","dataset(Housingdata.csv) : https://www.kaggle.com/datasets/altavish/boston-housing-dataset\n","\n","traget: MEDV(집값)  \n","input feature: MEDV를 제외한 모든 값  \n","전체 데이터 약 500개중 400개까지 train data로 사용, 나머지는 test data  \n","\n","각 data를 torch.utils.data.Dataset을 통해 하나씩 load할 수 있는 class구현  \n","이후 torch.utils.data.DataLoader를 통해 batch개씩 변환\n","\n","torch.utils.data.Dataset의 주요 method \n","__ init __: 클래스를 오브젝트로 생성할때 불러와지는 함수, 클래스에서 필요한 인스턴스(데이터셋, 데이터경로)등을 생성  \n","__ len __: 해당 클래스에서 다루는 dataset의 길이를 반환하는 함수  \n","__ getitem __(index): index에 해당하는 데이터 하나를 tensor형태로 반환하는 함수"],"metadata":{"id":"_pyaoU4IYAaw"}},{"cell_type":"code","source":["from sklearn.datasets import load_boston\n","import pandas as pd\n","from torch.utils.data import Dataset, DataLoader\n","import torch\n","\n","from sklearn.preprocessing import MinMaxScaler\n","\n","\n","data_path = \"/content/drive/MyDrive/랩세미나(new)/tutorial/open_tutorial_DL_NLP/2일차/dataset/HousingData.csv\"\n","batch_size = 3\n","\n","#torch.utils.data.Dataset을 상속하여 Dataset class선언\n","class myDataset(Dataset):\n","  #오브젝트를 선언할때 불러오는 함수, superclass(부모클래스)의 init을 실행해 주어야함\n","  def __init__(self, df_data) -> None:\n","    super().__init__()\n","    self.df_data = df_data #self를 사용하면 class내부에서 __init__ method만이 아닌 다른 method에서도 사용가능\n","    self.data_y = df_data.loc[:,[\"MEDV\"]]\n","    self.data_x = df_data.drop([\"MEDV\"], axis=1)\n","      \n","  #list 형태의 class를 만들때 필수로 사용되는 함수, 전체 길이를 알아야 인덱싱이 가능\n","  def __len__(self):\n","    return len(self.data_y)\n","\n","  #index에 해당하는 데이터를 반환해주는 함수\n","  def __getitem__(self, index):\n","    data = torch.Tensor(self.data_x.loc[index,:])\n","    target = torch.Tensor(self.data_y.loc[index,:])\n","\n","    return data, target\n","\n","#data load후 train(400개)/test(나머지)데이터를 분할\n","data_df = pd.read_csv(data_path).dropna()\n","train_data_df = data_df.loc[:400,:].reset_index()\n","test_data_df = data_df.loc[400:,:].reset_index()\n","\n","#각 dataset을 선언\n","trainDataset = myDataset(train_data_df)\n","testDataset = myDataset(test_data_df)\n","\n","#선언된 dataset을 dataloader를 통해 batch processing\n","trainDataloader = DataLoader(trainDataset, batch_size = batch_size)\n","testDataloader = DataLoader(testDataset, batch_size = batch_size)\n","\n","#잘 작동하는지 test\n","for i in trainDataset:\n","  print(\"dataset test\")\n","  print(i)\n","  break\n","\n","#잘 작동하는지 test\n","for i in trainDataloader:\n","  print(\"data loader test\")\n","  data = i[0]\n","  target = i[1]\n","  print(data)\n","  print(data.shape)\n","  print(target)\n","  print(target.shape)\n","  break\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GgwV6nIIUapc","executionInfo":{"status":"ok","timestamp":1657076782299,"user_tz":-540,"elapsed":4,"user":{"displayName":"김균엽","userId":"17069744786161993916"}},"outputId":"fdbc04aa-93c6-4376-c2cb-e8d0080dba58"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["dataset test\n","(tensor([0.0000e+00, 6.3200e-03, 1.8000e+01, 2.3100e+00, 0.0000e+00, 5.3800e-01,\n","        6.5750e+00, 6.5200e+01, 4.0900e+00, 1.0000e+00, 2.9600e+02, 1.5300e+01,\n","        3.9690e+02, 4.9800e+00]), tensor([24.]))\n","data loader test\n","tensor([[0.0000e+00, 6.3200e-03, 1.8000e+01, 2.3100e+00, 0.0000e+00, 5.3800e-01,\n","         6.5750e+00, 6.5200e+01, 4.0900e+00, 1.0000e+00, 2.9600e+02, 1.5300e+01,\n","         3.9690e+02, 4.9800e+00],\n","        [1.0000e+00, 2.7310e-02, 0.0000e+00, 7.0700e+00, 0.0000e+00, 4.6900e-01,\n","         6.4210e+00, 7.8900e+01, 4.9671e+00, 2.0000e+00, 2.4200e+02, 1.7800e+01,\n","         3.9690e+02, 9.1400e+00],\n","        [2.0000e+00, 2.7290e-02, 0.0000e+00, 7.0700e+00, 0.0000e+00, 4.6900e-01,\n","         7.1850e+00, 6.1100e+01, 4.9671e+00, 2.0000e+00, 2.4200e+02, 1.7800e+01,\n","         3.9283e+02, 4.0300e+00]])\n","torch.Size([3, 14])\n","tensor([[24.0000],\n","        [21.6000],\n","        [34.7000]])\n","torch.Size([3, 1])\n"]}]},{"cell_type":"markdown","source":["Deep learning 모델 구현  \n","torch.nn.Module을 이용하여 모델 구현  \n","1st hidden layer의 feature는 100개  \n","2nd hidden layer의 feature는 10개  \n","인 모델을 구현한다.  \n","\n","torch.nn.Linear: perceptron의 weighted sum과 같이 linaer regression연산을 하는 calss  \n","torch.nn.ReLU: ReLU activation function을 수행하는 class  \n","\n","torch.nn.Module의 주요 method  \n","__ init __: 클래스를 오브젝트로 생성할때 불러와지는 함수, 클래스에서 필요한 인스턴스(사용할 deep learning layer, activation function, 등)등을 생성  \n","__ forward __(data): 입력받은 data를 딥러닝 모델을 통해 결과를 예측하여 반환하는 class  \n","\n","**각 딥러닝 레이어 연산 중 차원수를 확인하고 잘 맞춰줄 것**  \n","참고 document(해당 사이트의 shape를 확인하고 tensor형태 결정)  \n","nn.Linear: https://pytorch.org/docs/stable/generated/torch.nn.Linear.html  \n","nn.ReLU: https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html"],"metadata":{"id":"n7j-zH_mfTbY"}},{"cell_type":"code","execution_count":5,"metadata":{"id":"pxJqIF7cW-lT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657076791088,"user_tz":-540,"elapsed":275,"user":{"displayName":"김균엽","userId":"17069744786161993916"}},"outputId":"d79438e2-aaec-4ed9-945c-b38ff2aa0ff5"},"outputs":[{"output_type":"stream","name":"stdout","text":["model test\n","input data\n","tensor([[0.0000e+00, 6.3200e-03, 1.8000e+01, 2.3100e+00, 0.0000e+00, 5.3800e-01,\n","         6.5750e+00, 6.5200e+01, 4.0900e+00, 1.0000e+00, 2.9600e+02, 1.5300e+01,\n","         3.9690e+02, 4.9800e+00],\n","        [1.0000e+00, 2.7310e-02, 0.0000e+00, 7.0700e+00, 0.0000e+00, 4.6900e-01,\n","         6.4210e+00, 7.8900e+01, 4.9671e+00, 2.0000e+00, 2.4200e+02, 1.7800e+01,\n","         3.9690e+02, 9.1400e+00],\n","        [2.0000e+00, 2.7290e-02, 0.0000e+00, 7.0700e+00, 0.0000e+00, 4.6900e-01,\n","         7.1850e+00, 6.1100e+01, 4.9671e+00, 2.0000e+00, 2.4200e+02, 1.7800e+01,\n","         3.9283e+02, 4.0300e+00]])\n","output predict\n"," tensor([[2.1675],\n","        [1.9252],\n","        [1.3669]], grad_fn=<ReluBackward0>)\n","ground thruth\n","tensor([[24.0000],\n","        [21.6000],\n","        [34.7000]])\n"]}],"source":["from torch import nn\n","\n","#딥러닝 모델을 작성하기위한 모듈\n","class myModel(nn.Module):\n","  #오브젝트를 선언할 때 불러와지는 함수 일반적으로 이번 모델에서 사용될 각 레이어들이 포함됨\n","  def __init__(self) -> None:\n","      super().__init__()\n","\n","      #input_feature:14,  1st_hidden: 100, 2nd_hidden: 10, output: 1의 형태에 맞는 linear layer들을 선언, activation으로 Relu사용\n","      self.linear1 = nn.Linear(14,100, bias=True)\n","      self.linear2 = nn.Linear(100,10, bias=True)\n","      self.linear3 = nn.Linear(10,1, bias=True)\n","      self.relu = nn.ReLU()\n","    \n","  #데이터를 입력받고 딥러닝 연산후 결과를 반환하는 함수\n","  def forward(self, x):\n","      #print(x.shape)\n","      x = self.linear1(x)\n","      x = self.relu(x)\n","      #print(x.shape)\n","      x = self.linear2(x)\n","      x = self.relu(x)\n","      #print(x.shape)\n","      x = self.linear3(x)\n","      x = self.relu(x)\n","      #print(x.shape)\n","\n","      return x\n","\n","#작성한 모델 선언\n","model = myModel()\n","\n","#잘 작동하는지 test\n","for i in trainDataloader:\n","  print(\"model test\")\n","  data = i[0]\n","  target = i[1]\n","  \n","  print(\"input data\")\n","  print(data)\n","  print(\"output predict\\n\", model(data))\n","  print(\"ground thruth\")\n","  print(target)\n","  break\n"]},{"cell_type":"markdown","source":["작성한 dataset과 model을 이용하여 딥러닝 프로세스 구현  \n","\n","pytorch 딥러닝 프로세스\n","1. dataset, model선언\n","2. dataset과 model을 통한 결과 예측\n","3. 예측된 결과를 통해 **loss**연산 및 **loss.backward**\n","4. **optimizer.step()**를 사용하여 graident update\n","\n","주요 오브젝트  \n","torch.nn.MSELoss: 예측값과 정답을 통해 MSE값을 반환하는 class  \n","torch.optim.Adam: Adam optimizer를 통해 gradient update를 수행하는 class\n","\n","각 오브젝트의 입력과 선언은 다음 doc 참조:  \n","MSELoss: https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html  \n","Adam: https://pytorch.org/docs/stable/generated/torch.optim.Adam.html"],"metadata":{"id":"MvVKNpwAoymc"}},{"cell_type":"code","source":["from torch.optim import Adam\n","from torch.nn import MSELoss\n","\n","#학습을 위한 optimizer와 loss function 설정\n","optimizer = Adam(model.parameters(), lr=0.001)\n","lf = MSELoss()\n","\n","#100번의 에폭을 실행\n","for e in range(100):\n","  print(\"\\n\\nepoch \", e)\n","  epoch_loss = 0\n","  \n","  #선언한 모델 오브젝트를 학습가능한 상태로 변경\n","  model.train()\n","\n","  #모든 학습데이터에 대해서 학습\n","  for i in trainDataloader:\n","    #매 배치에 대한 gradient계산 이전에 optimizer에 저장된 이전 batch에 gradient를 삭제(초기화)\n","    optimizer.zero_grad()\n","    data = i[0]\n","    target = i[1]\n","\n","    #결과 도출\n","    output = model(data)\n","\n","    #loss연산\n","    loss = lf(output, target)\n","    #print(loss)\n","\n","    #loss backpropagation\n","    loss.backward()\n","\n","    #gradient update\n","    optimizer.step()\n","\n","    epoch_loss += loss.item()\n","  \n","  print(\"train loss\", epoch_loss/len(trainDataloader))\n","\n","  #model이 학습되지 않는 상태로 변경\n","  model.eval()\n","  test_loss = 0\n","\n","  #gradient를 계산하지 않도록 하여 cost낭비 방지\n","  with torch.no_grad():\n","    #모든 test dataset에 대해서 결과연산\n","    for i in testDataloader:\n","      data = i[0]\n","      target = i[1]\n","\n","      output = model(data)\n","\n","      loss = lf(output, target)\n","      test_loss += loss.item()\n","\n","  print(\"test loss\", test_loss/len(testDataloader))\n","    \n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"enENUuVFo4VN","executionInfo":{"status":"ok","timestamp":1657076833808,"user_tz":-540,"elapsed":28801,"user":{"displayName":"김균엽","userId":"17069744786161993916"}},"outputId":"95dbb4c9-f074-4ae0-d36d-9fc072726b92"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","epoch  0\n","train loss 103.50126444271632\n","test loss 21.90126775418009\n","\n","\n","epoch  1\n","train loss 83.68431367164567\n","test loss 90.38706636428833\n","\n","\n","epoch  2\n","train loss 70.20951399121965\n","test loss 113.92538489614215\n","\n","\n","epoch  3\n","train loss 74.71994888158072\n","test loss 71.44948433126721\n","\n","\n","epoch  4\n","train loss 82.32168143966368\n","test loss 45.24218084130968\n","\n","\n","epoch  5\n","train loss 79.4304623399462\n","test loss 58.67536563532693\n","\n","\n","epoch  6\n","train loss 71.39557461440563\n","test loss 65.09143807206836\n","\n","\n","epoch  7\n","train loss 74.63659432161423\n","test loss 61.42538489614214\n","\n","\n","epoch  8\n","train loss 70.91169275215694\n","test loss 62.11209160940988\n","\n","\n","epoch  9\n","train loss 67.88339177597136\n","test loss 50.10132004533495\n","\n","\n","epoch  10\n","train loss 65.79456045003164\n","test loss 49.773278129952296\n","\n","\n","epoch  11\n","train loss 60.46799822137469\n","test loss 47.097924379365786\n","\n","\n","epoch  12\n","train loss 56.42920647774424\n","test loss 43.068077084209236\n","\n","\n","epoch  13\n","train loss 49.99406764450527\n","test loss 40.841173014470506\n","\n","\n","epoch  14\n","train loss 52.45845407417843\n","test loss 40.67264346139772\n","\n","\n","epoch  15\n","train loss 44.55222909961428\n","test loss 37.03498804143497\n","\n","\n","epoch  16\n","train loss 43.37199809437706\n","test loss 37.52910004769053\n","\n","\n","epoch  17\n","train loss 41.04481334175382\n","test loss 37.13787949510983\n","\n","\n","epoch  18\n","train loss 40.32681022839887\n","test loss 37.49748455626624\n","\n","\n","epoch  19\n","train loss 40.41174849555606\n","test loss 38.6655512366976\n","\n","\n","epoch  20\n","train loss 38.889265828615144\n","test loss 41.235891248498646\n","\n","\n","epoch  21\n","train loss 38.773921911489396\n","test loss 38.76607870629856\n","\n","\n","epoch  22\n","train loss 38.262160355988\n","test loss 37.939695860658375\n","\n","\n","epoch  23\n","train loss 38.64539113215038\n","test loss 38.21963818158422\n","\n","\n","epoch  24\n","train loss 37.90067776257084\n","test loss 37.93903321027756\n","\n","\n","epoch  25\n","train loss 37.88281844158967\n","test loss 38.99254351002829\n","\n","\n","epoch  26\n","train loss 37.294347201997326\n","test loss 35.53026510987963\n","\n","\n","epoch  27\n","train loss 35.1272602020275\n","test loss 35.37709070955004\n","\n","\n","epoch  28\n","train loss 34.13452220850048\n","test loss 34.98047000169754\n","\n","\n","epoch  29\n","train loss 33.214720779941196\n","test loss 34.49683837379728\n","\n","\n","epoch  30\n","train loss 32.163002338863556\n","test loss 34.476330637931824\n","\n","\n","epoch  31\n","train loss 31.54594723951249\n","test loss 34.01227521896362\n","\n","\n","epoch  32\n","train loss 30.880108760368255\n","test loss 33.39911646502359\n","\n","\n","epoch  33\n","train loss 30.696047828594843\n","test loss 30.86895705120904\n","\n","\n","epoch  34\n","train loss 30.426181539467404\n","test loss 30.28797756774085\n","\n","\n","epoch  35\n","train loss 30.004117988972435\n","test loss 27.421643563679286\n","\n","\n","epoch  36\n","train loss 29.455532109169734\n","test loss 26.22077979360308\n","\n","\n","epoch  37\n","train loss 29.266264926819574\n","test loss 24.606064615505083\n","\n","\n","epoch  38\n","train loss 29.050954549653188\n","test loss 23.950719087251596\n","\n","\n","epoch  39\n","train loss 28.884245617049082\n","test loss 23.912694945000112\n","\n","\n","epoch  40\n","train loss 28.912793994517553\n","test loss 22.641693241894245\n","\n","\n","epoch  41\n","train loss 28.76699166893959\n","test loss 21.858379574758665\n","\n","\n","epoch  42\n","train loss 27.946117824032193\n","test loss 22.63320607053382\n","\n","\n","epoch  43\n","train loss 27.249984619730995\n","test loss 23.372211570186273\n","\n","\n","epoch  44\n","train loss 26.755319509052097\n","test loss 23.903012647160462\n","\n","\n","epoch  45\n","train loss 26.55379559454464\n","test loss 24.51345164648124\n","\n","\n","epoch  46\n","train loss 25.99277100676582\n","test loss 19.130839266947337\n","\n","\n","epoch  47\n","train loss 25.558750613246644\n","test loss 18.840227834880352\n","\n","\n","epoch  48\n","train loss 24.97701810144243\n","test loss 19.102555114100273\n","\n","\n","epoch  49\n","train loss 24.627968971786046\n","test loss 18.367461689880916\n","\n","\n","epoch  50\n","train loss 24.101751093282587\n","test loss 19.37637693328517\n","\n","\n","epoch  51\n","train loss 23.809536636798153\n","test loss 19.234177819320134\n","\n","\n","epoch  52\n","train loss 23.38115727585696\n","test loss 19.127898490854673\n","\n","\n","epoch  53\n","train loss 23.443151688220954\n","test loss 21.562010407447815\n","\n","\n","epoch  54\n","train loss 22.66903416230565\n","test loss 22.85737555367606\n","\n","\n","epoch  55\n","train loss 22.528722967704137\n","test loss 23.242450598228192\n","\n","\n","epoch  56\n","train loss 21.75613187842426\n","test loss 23.10169949701854\n","\n","\n","epoch  57\n","train loss 21.608328247283186\n","test loss 22.69859756742205\n","\n","\n","epoch  58\n","train loss 21.855260550337178\n","test loss 24.99401719229562\n","\n","\n","epoch  59\n","train loss 21.11212674911533\n","test loss 25.716763249465398\n","\n","\n","epoch  60\n","train loss 22.08632892199925\n","test loss 26.992964267730713\n","\n","\n","epoch  61\n","train loss 21.465410715057736\n","test loss 29.835609674453735\n","\n","\n","epoch  62\n","train loss 21.138391383063226\n","test loss 25.8277998481478\n","\n","\n","epoch  63\n","train loss 22.148603626659938\n","test loss 25.389583902699606\n","\n","\n","epoch  64\n","train loss 21.90048557746978\n","test loss 28.824970820120402\n","\n","\n","epoch  65\n","train loss 21.7302309629463\n","test loss 28.513146221637726\n","\n","\n","epoch  66\n","train loss 22.549897571688607\n","test loss 28.309089285986765\n","\n","\n","epoch  67\n","train loss 22.28234687305632\n","test loss 29.1241341148104\n","\n","\n","epoch  68\n","train loss 21.87723639891261\n","test loss 29.098690816334315\n","\n","\n","epoch  69\n","train loss 22.178260830470492\n","test loss 32.14526220304625\n","\n","\n","epoch  70\n","train loss 22.081712998095014\n","test loss 31.86809544478144\n","\n","\n","epoch  71\n","train loss 22.055033319478945\n","test loss 32.30681220122746\n","\n","\n","epoch  72\n","train loss 21.478700298851443\n","test loss 32.75157214488302\n","\n","\n","epoch  73\n","train loss 21.479053498024033\n","test loss 34.538488251822336\n","\n","\n","epoch  74\n","train loss 21.655532925043786\n","test loss 33.381873888628824\n","\n","\n","epoch  75\n","train loss 21.607441007026605\n","test loss 33.54905761565481\n","\n","\n","epoch  76\n","train loss 21.675834171190147\n","test loss 33.965150656444685\n","\n","\n","epoch  77\n","train loss 21.546321287538323\n","test loss 33.69162964820862\n","\n","\n","epoch  78\n","train loss 21.964671996945427\n","test loss 35.54723847976753\n","\n","\n","epoch  79\n","train loss 22.446673519235283\n","test loss 37.154154249600005\n","\n","\n","epoch  80\n","train loss 22.07857544819514\n","test loss 35.57475186245782\n","\n","\n","epoch  81\n","train loss 22.27337079896104\n","test loss 37.68782320192882\n","\n","\n","epoch  82\n","train loss 22.731700873445895\n","test loss 32.827641112463816\n","\n","\n","epoch  83\n","train loss 23.24608260556346\n","test loss 33.64250411518982\n","\n","\n","epoch  84\n","train loss 23.07374424097084\n","test loss 36.162521711417604\n","\n","\n","epoch  85\n","train loss 24.04427108849798\n","test loss 33.16814706155232\n","\n","\n","epoch  86\n","train loss 23.373468373786835\n","test loss 34.13280071531023\n","\n","\n","epoch  87\n","train loss 22.83160304384572\n","test loss 37.12815402661051\n","\n","\n","epoch  88\n","train loss 23.111659787098567\n","test loss 39.198286056518555\n","\n","\n","epoch  89\n","train loss 25.62895127861273\n","test loss 35.48246884346008\n","\n","\n","epoch  90\n","train loss 25.67240269978841\n","test loss 39.66813893829073\n","\n","\n","epoch  91\n","train loss 23.464199382776307\n","test loss 41.794031883989064\n","\n","\n","epoch  92\n","train loss 24.28671984317757\n","test loss 42.69822365471295\n","\n","\n","epoch  93\n","train loss 24.848199594872337\n","test loss 42.91488759858267\n","\n","\n","epoch  94\n","train loss 24.842450267218407\n","test loss 43.81065239650862\n","\n","\n","epoch  95\n","train loss 24.607987997219677\n","test loss 47.215829351118636\n","\n","\n","epoch  96\n","train loss 24.566199718273822\n","test loss 48.40569578324045\n","\n","\n","epoch  97\n","train loss 25.542695840909367\n","test loss 51.28425568342209\n","\n","\n","epoch  98\n","train loss 25.350353494428454\n","test loss 50.258444083588465\n","\n","\n","epoch  99\n","train loss 25.286659943489802\n","test loss 52.195299621139256\n"]}]}]}